["```js\\1\n\nOf course, the program pauses at this point while the database layer sends the query to the database and waits for the result or the error. This is an example of a blocking function call. Depending on the query, this pause can be quite long (well, a few milliseconds, which is ages in computer time). This pause is bad because the execution thread can do nothing while it waits for the result to arrive. If your software is running on a single-threaded platform, the entire server would be blocked and unresponsive. If instead your application is running on a thread-based server platform, a thread-context switch is required to satisfy any other requests that arrive. The greater the number of outstanding connections to the server, the greater the number of thread-context switches. Context switching is not free because more threads require more memory per thread state and more time for the CPU to spend on thread management overheads.\n\nThe key inspiration guiding the original development of Node.js was the simplicity of a single-threaded system. A single execution thread means that the server doesn't have the complexity of multithreaded systems. This choice meant that Node.js required an event-driven model for handling concurrent tasks. Instead of the code waiting for results from a blocking request, such as retrieving data from a database, an event is\u00a0instead\u00a0dispatched to an event handler.\n\nUsing threads to implement concurrency often comes with admonitions, such as *expensive and error-prone*,\u00a0*the error-prone synchronization primitives of Java*, or\u00a0*designing concurrent software can be complex and error-prone*. The complexity comes from access to shared variables and various strategies to avoid deadlock and competition between threads. The\u00a0*synchronization primitives of Java*\u00a0are an example of such a strategy, and obviously many programmers find them difficult to use. There's a tendency to create frameworks such as\u00a0`java.util.concurrent`\u00a0to tame the complexity of threaded concurrency, but some argue that papering over complexity only makes things more complex.\u00a0\n\nA typical Java programmer might object at this point. Perhaps their application code is written against a framework such as Spring, or maybe they're directly using Java EE. In either case, their application code does not use concurrency features or deal with threads, and therefore where is the complexity that we just described? Just because that complexity is hidden within Spring and Java EE does not mean that there is no complexity and overhead.\n\nOkay, we get it: while multithreaded systems can do amazing things, there is inherent complexity. What does Node.js offer?\n\n## The Node.js answer to complexity\n\nNode.js asks us to think differently about concurrency. Callbacks fired asynchronously from an event loop are a much simpler concurrency model\u2014simpler to understand, simpler to implement, simpler to reason about, and simpler to debug and maintain.\u00a0\n\nNode.js has a single execution thread with no waiting on I/O or context switching. Instead, there is an event loop that dispatches\u00a0events to handler functions as things happen. A request that would have blocked the execution thread instead executes asynchronously, with the results or errors triggering an event. Any operation that would block or otherwise take time to complete must use the asynchronous model.\u00a0\n\nThe original Node.js paradigm delivered the dispatched event to an anonymous function. Now that JavaScript has `async` functions, the Node.js paradigm is shifting to deliver results and errors via a promise that is handled by the `await` keyword. When an asynchronous function is called, control quickly passes to the event loop rather than causing Node.js to block. The event loop continues handling the variety of events while recording where to send each result or error.\n\nBy using an asynchronous event-driven I/O, Node.js removes most of this overhead while introducing very little of its own.\n\nOne of the points Ryan Dahl made in the Cinco de Node presentation is a hierarchy of execution time for different requests.\u00a0Objects in memory are more quickly accessed (in the order of nanoseconds) than objects on disk or objects retrieved over the network (milliseconds or seconds). The longer access time for external objects is measured in zillions of clock cycles, which can be an eternity when your customer is sitting at their web browser ready to move on if it takes longer than two seconds to load the page.\u00a0\n\nTherefore, concurrent request handling means using a strategy to handle the requests that take longer to satisfy. If the goal is to avoid the complexity of a multithreaded system, then the system must use asynchronous operations as Node.js does.\n\nWhat do these asynchronous function calls look like?\n\n## Asynchronous requests in Node.js\n\nIn Node.js, the query that we looked at previously will read as follows:\n\n```", "```js\\1\n\nThis is a little better, especially in instances of deeply nested event handling.\n\nThe big advance came with the ES-2017 `async` function:\n\n```", "```js\\1\n\nThis is one of the simpler web servers that you\u00a0can build with Node.js. The `http` object encapsulates the HTTP protocol, and its `http.createServer` method creates a whole web server, listening on the port specified in the `listen` method. Every request (whether a `GET` or `POST` on any URL) on that web server calls the provided function. It is very simple and lightweight. In this case, regardless of the URL, it returns a simple `text/plain`\u00a0that is the\u00a0`Hello World` response.\n\nRyan Dahl showed a simple benchmark in a video titled *Ryan Dahl: Introduction to Node.js*\u00a0(on the YUI Library channel on YouTube,\u00a0[https://www.youtube.com/watch?v=M-sc73Y-zQA](https://www.youtube.com/watch?v=M-sc73Y-zQA)). It used a similar HTTP server to this, but that returned a one-megabyte binary buffer; Node.js gave 822 req/sec, while Nginx gave 708 req/sec, for a 15% improvement over Nginx. He also noted that Nginx peaked at four megabytes of memory, while Node.js peaked at 64 megabytes.\u00a0\n\nThe key observation was that Node.js, running an interpreted, JIT-compiled, high-level language, was about as fast as Nginx, built of highly optimized C code, while running similar tasks. That presentation was in May 2010, and Node.js has improved hugely since then, as shown in\u00a0Chris Bailey's talk that we referenced earlier.\n\nYahoo! search engineer Fabian Frank published a performance case study of a real-world search query suggestion widget implemented with Apache/PHP and two variants of Node.js stacks ([http://www.slideshare.net/FabianFrankDe/nodejs-performance-case-study](http://www.slideshare.net/FabianFrankDe/nodejs-performance-case-study)). The application is a pop-up panel showing search suggestions as the user types in phrases using a JSON-based HTTP query. The Node.js version could handle eight times the number of requests per second with the same request latency. Fabian Frank said both Node.js stacks scaled linearly until CPU usage hit 100%.\u00a0\n\nLinkedIn did a massive overhaul of their mobile app using Node.js for the server-side to replace an old Ruby on Rails app. The switch lets them move from 30 servers down to 3, and allowed them to merge the frontend and backend team because everything was written in JavaScript. Before choosing Node.js, they'd evaluated Rails with Event Machine, Python with Twisted, and Node.js, chose Node.js for the reasons that we just discussed. For a look at what LinkedIn did, see [http://arstechnica.com/information-technology/2012/10/a-behind-the-scenes-look-at-linkedins-mobile-engineering/](http://arstechnica.com/information-technology/2012/10/a-behind-the-scenes-look-at-linkedins-mobile-engineering/).\n\nMost existing Node.js performance tips tend to have been written for older V8 versions that used the CrankShaft optimizer. The V8 team has completely dumped CrankShaft, and it has a new optimizer called TurboFan\u2014for example, under CrankShaft, it was slower to use `try/catch`, `let/const`, generator functions, and so on. Therefore, common wisdom said to not use those features, which is depressing because we want to use the new JavaScript features because of how much it has improved the JavaScript language. Peter Marshall, an engineer on the V8 team at Google, gave a talk at Node.js Interactive 2017 claiming that, using TurboFan, you should just write natural JavaScript. With TurboFan, the goal is for across-the-board performance improvements in V8\\. To view the presentation, see the video titled *High Performance JS* in V8\u00a0at\u00a0[https://www.youtube.com/watch?v=YqOhBezMx1o](https://www.youtube.com/watch?v=YqOhBezMx1o).\n\nA truism about JavaScript is that it's no good for heavy computation work because of the nature of JavaScript. We'll go over some ideas that are related to this in the next section. A talk by Mikola Lysenko at Node.js Interactive 2016 went over some issues with numerical computing in JavaScript, and some possible solutions. Common numerical computing involves large numerical arrays processed by numerical algorithms that you might have learned in calculus or linear algebra classes. What JavaScript lacks is multidimensional arrays and access to certain CPU instructions. The solution that he presented is a library to implement multidimensional arrays in JavaScript, along with another library full of numerical computing algorithms. To view the presentation, see the video titled *Numerical Computing in JavaScript* by Mikola Lysenko\u00a0at\u00a0[https://www.youtube.com/watch?v=1ORaKEzlnys](https://www.youtube.com/watch?v=1ORaKEzlnys)[.\u00a0](https://www.youtube.com/watch?v=1ORaKEzlnys)\n\nAt the Node.js Interactive conference in 2017, IBM's Chris Bailey made a case for Node.js being an excellent choice for highly scalable microservices. Key performance characteristics are I/O performance (measured in transactions per second), startup time (because that limits how quickly your service can scale up to meet demand), and memory footprint (because that determines how many application instances can be deployed per server). Node.js excels on all those measures; with every subsequent release, it either improves on each measure or remains fairly steady. Bailey presented figures comparing Node.js to a similar benchmark written in Spring Boot showing Node.js to perform much better. To view his talk, see the video titled *Node.js Performance and Highly Scalable Micro-Services - Chris Bailey, IBM*\u00a0at [https://www.youtube.com/watch?v=Fbhhc4jtGW4](https://www.youtube.com/watch?v=Fbhhc4jtGW4).\n\nThe bottom line is that Node.js excels at event-driven I/O throughput. Whether a Node.js program can excel at computational programs depends on your ingenuity in working around some limitations in the JavaScript language.\n\nA big problem with computational programming is that it prevents the event loop from executing. As we will see in the next section, that can make Node.js look like a poor candidate for anything.\n\n### Is Node.js a cancerous scalability disaster?\n\nIn October 2011, a blog post (since pulled from the blog where it was published) titled *Node.js is a cancer* called Node.js a scalability disaster. The example shown for proof was a CPU-bound implementation of the Fibonacci sequence algorithm. While the argument was flawed\u2014since nobody implements Fibonacci that way\u2014it made the valid point that Node.js application developers have to consider the following: where do you put the heavy computational tasks?\n\nA key to maintaining high throughput of Node.js applications is by ensuring that events are handled quickly. Because it uses a single execution thread, if that thread is bogged down with a big calculation, Node.js cannot handle events, and event throughput will suffer.\n\nThe Fibonacci sequence, serving as a stand-in for heavy computational tasks, quickly becomes computationally expensive to calculate for a na\u00efve implementation such as this:\n\n```", "```js\\1\n\nThis is an extension of the simple web server shown earlier. It looks in the request URL for an argument, `n`, for which to calculate the Fibonacci number. When it's calculated, the result is returned to the caller.\n\nFor sufficiently large values of `n` (for example, `40`), the server becomes completely unresponsive because the event loop is not running. Instead, this function has blocked event processing because the event loop cannot dispatch events while the function is grinding through the calculation.\n\nIn other words, the Fibonacci function is a stand-in for any blocking operation.\n\nDoes this mean that Node.js is a flawed platform? No, it just means that the programmer must take care to identify code with long-running computations and develop solutions. These include rewriting the algorithm to work with the event loop, rewriting the algorithm for efficiency, integrating a native code library, or\u00a0foisting computationally expensive calculations to a backend server.\n\nA simple rewrite dispatches the computations through the event loop, letting the server continue to handle requests on the event loop. Using callbacks and closures (anonymous functions), we're able to maintain asynchronous I/O and concurrency promises, as shown in the following code:\n\n```", "```js\\1\n\nWe've added a callback function to receive the result. In this case, the server is able to handle multiple Fibonacci number requests. But there is still a performance issue because of the inefficient algorithm.\n\nLater in this book, we'll explore this example a little more deeply to explore alternative approaches.\n\nIn the meantime, we can discuss why it's important to use efficient software stacks.\n\n## Server utilization, overhead costs, and environmental impact\n\nThe striving for optimal efficiency (handling more requests per second) is not just about the geeky satisfaction that comes from optimization. There are real business and environmental benefits. Handling more requests per second, as Node.js servers can do, means the difference between buying lots of servers and buying only a few servers. Node.js potentially lets your organization do more with less.\n\nRoughly speaking, the more servers you buy, the greater the monetary cost and the greater the environmental cost. There's a whole field of expertise around reducing costs and the environmental impact of running web-server facilities to which that rough guideline doesn't do justice. The goal is fairly obvious\u2014fewer servers, lower costs, and a lower environmental impact by using more efficient software.\n\nIntel's paper, *Increasing Data Center Efficiency with Server Power Measurements*\u00a0([https://www.intel.com/content/dam/doc/white-paper/intel-it-data-center-efficiency-server-power-paper.pdf](https://www.intel.com/content/dam/doc/white-paper/intel-it-data-center-efficiency-server-power-paper.pdf)), gives an objective framework for understanding efficiency and data center costs. There are many factors, such as buildings, cooling systems, and computer system designs. Efficient building design, efficient cooling systems, and efficient computer systems (data center efficiency, data center density, and storage density) can lower costs and environmental impact. But you can destroy these gains by deploying an inefficient software stack, compelling you to buy more servers than you would if you had an efficient software stack. Alternatively, you can amplify gains from data center efficiency with an efficient software stack that lets you decrease the number of servers required.\n\nThis talk about efficient software stacks isn't just for altruistic environmental purposes. This is one of those cases where being green can help your business bottom line.\n\nIn this section, we have learned a lot about how Node.js architecture differs from other programming platforms. The choice to eschew threads to implement concurrency simplifies away the complexity and overhead that comes from using threads. This seems to have fulfilled the promise of being more efficient. Efficiency has a number of benefits to many aspects of a business.\n\n# Embracing advances in the JavaScript language\n\nThe last couple of years have been an exciting time for JavaScript programmers. The TC-39 committee that oversees the ECMAScript standard has added many new features, some of which are syntactic sugar, but several of which have propelled us into a whole new era of JavaScript programming. By itself, the `async/await` feature promises us a way out of what's called callback fell, the situation that we find ourselves in when nesting callbacks within callbacks. It's such an important feature that it should necessitate a broad rethinking of the prevailing callback-oriented paradigm in Node.js and the rest of the JavaScript ecosystem.\n\nA few pages ago, you saw this:\n\n```"]