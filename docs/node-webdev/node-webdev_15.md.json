["```js\\1\n\nYou will receive a file that looks like this. These are the security tokens that identify your account. Don't worry, as no secrets are being leaked in this case. Those particular credentials have been revoked. The good news is that you can revoke these credentials at any time and download new credentials.\n\nNow that we have the credentials file, we can configure an AWS CLI profile.\n\nThe\u00a0`aws configure`\u00a0command, as the name implies, takes care of configuring your AWS CLI environment. This asks a series of questions, the first two of which are those keys. The interaction looks like this:\n\n```", "```js\\1\n\nThe AWS **Simple Storage Service** (**S3**) is a cloud file-storage system, and we are running these commands solely to verify the\u00a0correct installation of\u00a0the credentials.\u00a0 The\u00a0`ls`\u00a0command lists any files you have stored in S3\\. We don't care about the files that may or may not be in an S3 bucket, but whether this executes without error.\n\nThe first command shows us that execution with no\u00a0`--profile`\u00a0option, and no `default` profile, produces an error. If there were a\u00a0`default`\u00a0AWS profile, that would have been used. However, we did not create a `default` profile, so therefore no profile was available and we got an error. The second shows the same command with an explicitly named profile. The third shows the `AWS_PROFILE` environment variable being used to name the profile to be deployed.\n\nUsing the environment variables supported by the AWS CLI tool, such as\u00a0`AWS_PROFILE`, lets us skip using command-line\u00a0options such as\u00a0`--profile`\u00a0while still being explicit about which profile to use.\n\nAs we said earlier, it is important that we interact with AWS via an IAM user, and therefore we must learn how to create an IAM user account. Let's do that next.\n\n## Creating an IAM user account, groups, and roles\n\nWe could do everything in this chapter using our root account but, as we said, that's bad form. Instead, it is recommended to create a second user\u2014an IAM user\u2014and give it only the permissions required by that user.\u00a0\n\nTo get to the IAM dashboard, click on\u00a0Services\u00a0in the navigation bar, and enter `IAM`. IAM stands for\u00a0Identity and Access Management. Also, the\u00a0My Security Credentials\u00a0dashboard is part of the IAM service, so we are probably already in the IAM area.\n\nThe first task is to create a role. In AWS, roles are used to associate privileges with a user account. You can create roles with extremely limited privileges or an extremely broad range of privileges.\n\nIn the IAM dashboard, you'll find a navigation menu on the left. It has sections for users, groups, roles, and other identity management topics. Click on the Roles choice. Then, in the Roles area, click on\u00a0Create Role. Perform the following steps:\n\n1.  Under\u00a0Type of trusted identity,\u00a0select\u00a0Another AWS account. Enter the account ID, which you will have recorded earlier while familiarizing yourself with the AWS account.\u00a0Then, click on\u00a0Next.\n2.  On the next page, we select the permissions for this role. For our purpose, select\u00a0`AdministratorAccess`, a privilege that grants full access to the AWS account. Then, click on\u00a0Next.\n3.  On the next page, you can add tags to the role. We don't need to do this, so click\u00a0Next.\n4.  On the last page, we give a name to the role. Enter\u00a0`admin`\u00a0because this role has administrator permissions. Click on\u00a0Create Role.\n\nYou'll see that the role,\u00a0admin, is now listed in the Role dashboard. Click on\u00a0admin\u00a0and you will be taken to a page where you can customize the role further. On this page, notice the characteristic named\u00a0Role ARN. Record this **Amazon Resource Name** (**ARN**) for future reference.\n\nARNs are identifiers used within AWS. You can reliably use this ARN in any area of AWS where we can specify a role. ARNs are used with almost every AWS resource.\n\nNext, we have to create an administrator group. In IAM, users are assigned to groups as a way of passing roles and other attributes to a group of IAM user accounts. To do this, perform the following steps:\n\n1.  In the left-hand navigation menu, click on Group, and then, in the group dashboard, click on\u00a0Create Group.\u00a0\n2.  For the group name, enter\u00a0`Administrators`.\u00a0\n3.  Skip the\u00a0Attach Policy\u00a0page, click\u00a0Next Step*,* and then, on the\u00a0Review\u00a0page, simply click\u00a0Create Group.\n4.  This creates a group with no permissions and directs you back to the group dashboard.\u00a0\n5.  Click on the Administrators group, and you'll be taken to the overview page. Record the ARN for the group.\n6.  Click on Permissions to open that tab, and then click on the\u00a0Inline policies\u00a0section header. We will be creating an inline policy, so click on the\u00a0Click here\u00a0link.\n7.  Click on\u00a0Custom Policy, and you'll be taken to the policy editor.\n8.  For the policy name, enter\u00a0`AssumeAdminRole`. Below that is an area where we enter a block of **JavaScript Object Notation** (**JSON**) code describing the policy. Once that's done, click the\u00a0Apply Policy\u00a0button.\n\nThe policy document to use is as follows:\n\n```", "```js\\1\n\nThis will create another AWS CLI profile, this time for the\u00a0`notes-app`\u00a0IAM user.\n\nUsing the AWS CLI, we can list the users in our account, as follows:\n\n```", "```js\\1\n\nSubstitute here the pathname where your browser downloaded the file.\n\nFor now, just make sure this file is correctly stored somewhere. When we deploy EC2 instances, we'll talk more about how to use it.\n\nWe have familiarized ourselves with the AWS Management Console, and created for ourselves an IAM user account. We have proved that we can log in to the console using the sign-in URL. While doing that, we copied down the AWS access credentials for the account.\n\nWe have completed the setup of the AWS command-line tools and user accounts. The next step is to set up Terraform.\n\n# An overview of the AWS infrastructure to be deployed\n\nAWS is a complex platform with dozens of services available to us. This project will touch on only the part required to deploy Notes as a Docker swarm on EC2 instances. In this section, let's talk about the infrastructure and AWS services we'll put to use.\n\nAn AWS VPC is what it sounds like\u2014namely, a service within AWS where you build your own private cloud service infrastructure. The AWS team designed the VPC service to look like something that you would construct in your own data center, but implemented on the AWS infrastructure. This means that the VPC is a container to which everything else we'll discuss is attached.\n\nThe AWS infrastructure is spread across the globe into what AWS calls regions. For example, `us-west-1` refers to Northern California, `us-west-2` refers to Oregon, and `eu-central-1` refers to Frankfurt. For production deployment, it is recommended\u00a0to use a region nearer your customers, but for experimentation, it is good to use the region closest to you. Within each region, AWS further subdivides its infrastructure into **availability zones** (a.k.a. **AZs**). An AZ might correspond to a specific building at an AWS data center site, but AWS often recommends that we deploy infrastructure to multiple AZs for reliability. In case one AZ goes down, the service can continue in the AZs that are running.\n\nWhen we allocate a VPC, we specify an address range for resources deployed within the VPC. The address range is specified with a **Classless Inter-Domain Routing** (**CIDR**) specifier. These are written as\u00a0`10.3.0.0/16` or `10.3.20.0/24`, which means any **Internet Protocol version 4** (**IPv4**) address starting with `10.3` and `10.3.20`, respectively.\n\nEvery device we attach to a VPC will be attached to a subnet, a virtual object similar to an Ethernet segment. Each subnet will be assigned a CIDR from the main range. A VPC assigned the\u00a0`10.3.0.0/16`\u00a0CIDR might have a subnet with a CIDR of\u00a0`10.3.20.0/24`. Devices attached to the subnet will have an IP address assigned within the range indicated by the CIDR for the subnet.\n\nEC2 is AWS's answer to a VPS\u00a0that you might rent from any web hosting provider. An EC2 instance is a virtual computer in the same sense that Multipass or VirtualBox lets you create a virtual computer on your laptop. Each EC2 instance is assigned a **central processing unit** (**CPU**), memory, disk capacity, and at least one network interface. Hence, an EC2 instance is attached to a subnet and is assigned an IP address from the subnet's assigned range.\n\nBy default, a device attached to a subnet has no internet access. The internet gateway\u00a0and **network address translation** (**NAT**) gateway\u00a0resources on AWS play a critical role in connecting resources attached to a VPC via the internet. Both are what is known as an internet router, meaning that both handle the routing of internet traffic from one network to another. Because a VPC contains a VPN, these gateways handle traffic between that network and the public internet, as follows:\n\n*   **Internet gateway**: This\u00a0handles two-way routing, allowing a resource allocated in a VPC to be reachable from the public internet. An internet gateway allows external traffic to enter the VPC, and it also allows resources in the VPC to access resources on the public internet.\n\n*   **NAT gateway**: This\u00a0handles one-way routing, meaning that resources on the VPC will be able to access resources on the public internet, but does not allow external traffic to enter the VPC.\u00a0To understand the NAT gateway, think about a common home Wi-Fi router because they also contain a NAT gateway. Such a gateway will manage a local IP address range such as\u00a0`192.168.0.0/16`, while the **internet service provider** (**ISP**) might assign a public IP address such as\u00a0`107.123.42.231`\u00a0to the connection. Local IP addresses, such as\u00a0`192.168.1.45`, will be assigned to devices connecting to the NAT gateway. Those local IP addresses do not appear in packets sent to the public internet. Instead, the NAT gateway translates the IP addresses to the public IP address of the gateway, and then when reply packets arrive, it translates the IP address to that of the local device. NAT translates IP addresses from the local network to the IP address of the NAT gateway.\u00a0\n\nIn practical terms, this determines the difference between a private subnet and a public subnet. A public subnet has a routing table that sends traffic for the public internet to an internet gateway, whereas a private subnet sends its public internet traffic to a NAT gateway.\n\nRouting tables describe how to route internet traffic. Inside any internet router, such as an internet gateway or a NAT gateway, is a function that determines how to handle internet packets destined for a location other than the local subnet. The routing function matches the destination address against routing table entries, and each routing table entry says where to forward matching packets.\n\nAttached to each device deployed in a VPC is a\u00a0security group. A security group is a firewall controlling what kind of internet traffic can enter or leave that device. For example, an EC2 instance might have a web server supporting HTTP (port `80`) and HTTPS (port `443`) traffic, and the administrator might also require SSH access (port `22`) to the instance. The security group would be configured to allow traffic from any IP address on ports `80` and `443`\u00a0and to allow traffic on port `22` from IP address ranges used by the administrator.\n\nA network **access control list** (**ACL**) is another kind of firewall that's attached to subnets. It, too, describes which traffic is allowed to enter or leave the subnet. The security groups and network ACLs are part of the security protections provided by AWS.\n\nIf a device connected to a VPC does not seem to work correctly, there might be an error in the configuration of these parts. It's necessary to check the security group attached to the device, and to the NAT gateway or internet gateway, and that the device is connected to the expected subnet, the routing table for the subnet, and any network ACLs.\n\n# Using Terraform to create an AWS infrastructure\n\nTerraform is an open source tool for configuring a cloud hosting infrastructure. It uses a declarative language to describe\u00a0the configuration of cloud services. Through a long list of plugins, called providers, it has support for\u00a0a variety of cloud services. In this chapter, we'll use Terraform to describe AWS infrastructure deployments.\n\nTo install Terraform, download an installer from\u00a0[https://www.terraform.io/downloads.html](https://www.terraform.io/downloads.html).\n\nAlternatively, you will find the Terraform CLI available in many package management systems.\n\nOnce installed, you can view the Terraform help with the following command:\n\n```", "```js\\1\n\nThe first word,\u00a0`resource`\u00a0or\u00a0`variable`, is the\u00a0block type, and in this case, we are declaring a\u00a0resource\u00a0and a\u00a0variable. Within the curly braces are the arguments to the block, and it is helpful to think of these as attributes.\n\nBlocks have labels\u2014in this case, the labels are\u00a0`aws_vpc`\u00a0and\u00a0`main`. We can refer to this specific resource elsewhere by joining the labels together as\u00a0`aws_vpc.main`. The name, `aws_vpc`, comes from the AWS provider and refers to VPC elements. In many cases, a block\u2014be it a resource or another kind\u2014will support attributes that can be accessed. For example, the CIDR for this VPC can be accessed as\u00a0`aws_vpc.main.cidr_block`.\n\nThe general structure is as follows:\n\n```", "```js\\1\n\nThis is what the\u00a0`variable`\u00a0and\u00a0`output`\u00a0declarations look like. Every value has a data type. For variables, we can attach a\u00a0description\u00a0to aid in their documentation. The declaration uses the word\u00a0`default`\u00a0rather than\u00a0`value`\u00a0because there are multiple ways (such as Terraform command-line arguments) to specify a value for a variable.\u00a0Terraform users can override the default value in several ways, such as the\u00a0`--var`\u00a0or\u00a0`--var-file`\u00a0command-line options.\n\nAnother type of value is\u00a0local. Locals exist only within a module because they are neither input values (variables) nor output values, as illustrated in the following code snippet:\n\n```", "```js\\1\n\nThis says to use the AWS provider plugin. It also configures this script to execute using the named AWS profile. Clearly, the AWS provider plugin requires AWS credential tokens in order to use the AWS API. It knows how to access the credentials file set up by\u00a0`aws configure`.\n\nTo learn more about configuring the AWS provider plugin, refer to\u00a0[https://www.terraform.io/docs/providers/aws/index.html](https://www.terraform.io/docs/providers/aws/index.html).\n\nAs shown here, the AWS plugin will look for the AWS credentials file in its default location, and use the\u00a0`notes-app`\u00a0profile name.\n\nIn addition, we have specified which AWS region to use. The reference,\u00a0`var.aws_region`,\u00a0is a Terraform variable. We use variables for any value that can legitimately vary. Variables can be easily customized to any value in several ways.\n\nTo support the variables, we create a file named\u00a0`variables.tf`, starting with this:\n\n```", "```js\\1\n\nThis initializes the current directory as a Terraform workspace. You'll see that it creates a directory,\u00a0`.terraform`, and a file named `terraform.tfstate`\u00a0containing data collected by Terraform. The `.tfstate` files are what is known as state files. These are in JSON format and store the data Terraform collects from the platform (in this case, AWS) regarding what has been deployed. State files must not be committed to source code repositories because it is possible for sensitive data to end up in those files. Therefore, a\u00a0`.gitignore`\u00a0file listing the state files is recommended.\n\nThe instructions say we should run\u00a0`terraform plan`, but before we do that, let's declare a few more things.\n\nTo declare the VPC and its related infrastructure, let's create a file named\u00a0`vpc.tf`. Start with the following command:\n\n```", "```js\\1\n\nThese values will be used throughout the project. For example,\u00a0`var.project_name` will be widely used as the basis for creating name tags for deployed resources.\n\nAdd the following to\u00a0`vpc.tf`:\n\n```", "```js\\1\n\nThis declares the internet gateway and the NAT gateway. Remember that internet gateways are used with public subnets, and NAT gateways are used with private subnets.\n\nAn **Elastic IP** (**EIP**) resource is how a public internet IP address is assigned. Any device that is to be visible to the public must be on a public subnet and have an EIP. Because the NAT gateway faces the public internet, it must have an assigned public IP address and an EIP.\n\nFor the subnets, create a file named `subnets.tf` containing the following:\n\n```", "```js\\1\n\nThese are the CIDRs corresponding to the resources declared earlier.\n\nFor these pieces to work together, we need appropriate routing tables to be configured. Create a file named `routing.tf` containing the following:\n\n```", "```js\\1\n\nThis command scans the Terraform files in the current directory and first determines that everything has the correct syntax, that all the values are known, and so forth. If any problems are encountered, it stops right away with error messages such as the following:\n\n```", "```js\\1\n\nWith\u00a0`terraform apply`,\u00a0the report shows the difference between the actual deployed state and the desired state as reflected by the Terraform files. In this case, there is no deployed state, so therefore everything that is in the files will be deployed. In other cases, you might have deployed a system and have made a change, in which case Terraform will work out which\u00a0changes have to be deployed based on the changes you've made. Once it calculates that, Terraform asks for permission to proceed. Finally, if we have said\u00a0yes, it will proceed and launch the desired infrastructure.\n\nOnce finished, it tells you what happened. One result is the values of the\u00a0`output`\u00a0commands in the scripts. These are both printed on the console and\u00a0are saved in the backend state file.\n\nTo see what was created, let's head to the AWS console and navigate to the VPC area, as follows:\n\n![](img/bc4b8bd0-aacc-43e7-a56f-4ff0618101aa.png)\n\nCompare the VPC ID in the screenshot with the one shown in the Terraform output, and you'll see that they match. What's shown here is the main routing table, and the CIDR, and other settings we made in our scripts. Every AWS account has a\u00a0default VPC that's presumably meant for experiments. It is a better form to create a VPC for each project so that resources for each project are separate from other projects.\n\nThe sidebar contains links for further dashboards for subnets, route tables, and other things, and an example dashboard can be seen in the following screenshot:\n\n![](img/89c715eb-9688-48ac-a145-33b8d9ed7835.png)\n\nFor example, this is the NAT gateway dashboard showing the one created for this project.\n\nAnother way to explore is with the AWS CLI tool. Just because we have Terraform doesn't mean we are prevented from using the CLI. Have a look at the following code block:\n\n```", "```js\\1\n\nTo focus on the subnets for a given VPC, we use the `--filters` option, passing in the filter named `vpc-id` and the VPC ID for which to filter.\n\nDocumentation for the AWS CLI can be found at\u00a0[https://docs.aws.amazon.com/cli/latest/reference/index.html](https://docs.aws.amazon.com/cli/latest/reference/index.html). [](https://docs.aws.amazon.com/cli/latest/reference/index.html) For documentation relating to the EC2 sub-commands, refer to\u00a0[https://docs.aws.amazon.com/cli/latest/reference/ec2/index.html](https://docs.aws.amazon.com/cli/latest/reference/ec2/index.html).\n\nThe AWS CLI tool has an extensive list of sub-commands and options. These are\u00a0enough to almost guarantee getting lost, so read carefully.\n\nIn this section, we learned how to use Terraform to set up the VPC and related infrastructure resources, and we also learned how to navigate both the AWS console and the AWS CLI to explore what had been created.\n\nOur next step is to set up an initial Docker Swarm cluster by deploying an EC2 instance to AWS.\n\n# Setting up a Docker Swarm cluster on AWS EC2\n\nWhat we have set up is essentially a blank slate. AWS has a long list of offerings that could be deployed to the VPC that we've created. What we're looking to do in this section is to set up a single EC2 instance to install Docker, and set up a single-node Docker Swarm cluster. We'll use this to familiarize ourselves with Docker Swarm. In the remainder of the chapter, we'll build more servers to create a larger swarm cluster for full deployment of Notes.\n\nA Docker Swarm cluster is simply a group of servers running Docker that have been joined together into a common pool. The code for the Docker Swarm orchestrator is bundled with the Docker Engine server but it is disabled by default. To create a swarm, we simply enable swarm mode by running `docker swarm init`\u00a0and then run a `docker swarm join` command on each system we want to be part of the cluster. From there, the Docker Swarm code automatically takes care of a long list of tasks. The features for Docker Swarm include the following:\n\n*   **Horizontal scaling**: When deploying a Docker service to a swarm, you tell it the desired number of instances as well as the memory and CPU requirements. The swarm takes that and computes the best distribution of tasks to nodes in the swarm.\n*   **Maintaining the desired state**: From the services deployed to a swarm, the swarm calculates the desired state of the system and tracks its current actual state. Suppose one of the nodes crashes\u2014the swarm will then readjust the running tasks to replace the ones that vaporized because of the crashed server.\n*   **Multi-host networking**: The overlay network driver automatically distributes network connections across the network of machines in the swarm.\n*   **Secure by default**: Swarm mode uses strong **Transport Layer Security** (**TLS**) encryption for all communication between nodes.\n*   **Rolling updates**: You can deploy an update to a service in such a manner where the swarm intelligently brings down existing service containers, replacing them with updated newer containers.\n\nFor an overview of Docker Swarm, refer to\u00a0[https://docs.docker.com/engine/swarm/](https://docs.docker.com/engine/swarm/).\n\nWe will use this section to not only learn how to set up a Docker Swarm but to also learn something about how Docker orchestration works.\n\nTo get started, we'll set up a single-node swarm on a single EC2 instance in order to learn some basics, before we move on to deploying a multi-node swarm and deploying the full Notes stack.\n\n## Deploying a single-node Docker Swarm on a single EC2 instance\n\nFor a quick introduction to Docker Swarm, let's start by installing Docker on a single EC2 node. We can kick the tires by trying a few commands and exploring the resulting system.\n\nThis will involve deploying Ubuntu 20.04 on an EC2 instance, configuring it to have the latest Docker Engine, and initializing swarm mode.\n\n### Adding an EC2 instance and configuring Docker\n\nTo launch an EC2 instance, we must first select which operating system to install. There are thousands of operating system configurations available. Each of these configurations is identified by an **AMI** code, where AMI stands for **Amazon Machine Image**.\n\nTo find your desired AMI, navigate to the EC2 dashboard on the AWS console. Then, click on the Launch Instance button, which starts a wizard-like interface to launch an instance. You can, if you like, go through the whole wizard since that is one way to learn about EC2 instances. We can search the AMIs via the first page of that wizard, where there is a search box.\n\nFor this exercise, we will use Ubuntu 20.04, so enter `Ubuntu` and then scroll down to find the correct version, as illustrated in the following screenshot:\n\n![](img/a3ad87a9-eb2c-4c6e-9ae8-995fd25d532d.png)\n\nThis is what the desired entry looks like. The AMI code starts with `ami-` and we see one version for x86 CPUs, and another for **ARM** (previously **Advanced RISC Machine**). ARM processors, by the way, are not just for your cell phone but are also used in servers. There is no need to launch an EC2 instance from here since we will instead do so with Terraform.\n\nAnother attribute to select is the instance size. AWS supports a long list of sizes that relate to the amount of memory, CPU cores, and disk space. For a chart of the available instance types, click on the Select button to proceed to the second page of the wizard, which shows a table of instance types and their attributes. For this exercise, we will use the `t2.micro` instance type because it is eligible for the free tier.\n\nCreate a file named `ec2-public.tf`\u00a0containing the following:\n\n```", "```js\\1\n\nThe AMI ID shown here is specifically for Ubuntu 20.04 in `us-west-2`. There will be other AMI IDs in other regions. The `key_pair` name shown here should be the key-pair name you selected when creating your key pair earlier.\n\nIt is not necessary to add the key-pair file to this directory, nor to reference the file you downloaded in these scripts. Instead, you simply give the name of the key pair. In our example, we named it `notes-app-key-pair`, and downloaded `notes-app-key-pair.pem`.\n\nThe `user_data` feature is very useful since it lets us customize an instance after creation. We're using this to automate the Docker setup on the instances. This field is to receive a string containing a shell script that will execute once the instance is launched. Rather than insert that script inline with the Terraform code, we have created a set of files that are shell script snippets. The Terraform `file` function reads the named file, returning it as a string. The Terraform `join` function takes an array of strings, concatenating them together with the delimiter character in between. Between the two we construct a shell script. The shell script first installs Docker Engine, then initializes Docker Swarm mode, and finally changes the hostname to help us remember that this is the public EC2 instance.\n\nCreate a directory named `sh` in which we'll create shell scripts, and in that directory create a file named `docker_install.sh`. To this file, add the following:\n\n```", "```js\\1\n\nThis is the security group declaration for the public EC2 instance. Remember that a security group describes the rules of a firewall that is attached to many kinds of AWS objects. This security group was already referenced in declaring `aws_instance.public`.\n\nThe main feature of security groups is the `ingress` and `egress` rules. As the words imply, `ingress` rules describe the network traffic allowed to enter the resource, and `egress` rules describe what's allowed to be sent by the resource. If you have to look up those words in a dictionary, you're not alone.\n\nWe have two `ingress` rules, and the first allows traffic on port `22`, which covers SSH traffic. The second allows traffic on port `80`, covering HTTP. We'll add more Docker rules later when they're needed.\n\nThe `egress` rule allows the EC2 instance to send any traffic to any machine on the internet.\n\nThese `ingress` rules are obviously very strict and limit the attack surface any miscreants can exploit.\n\nThe final task is to add these output declarations to\u00a0`ec2-public.tf`, as follows:\n\n```", "```js\\1\n\nIf the VPC infrastructure were already running, you would get output similar to this. The addition is two new objects, `aws_instance.public` and `aws_security_group.ec2-public-sg`. This looks good, so we proceed to deployment, as follows:\n\n```", "```js\\1\n\nOn a Linux or macOS system where we're using SSH, the command is as shown here. The `-i` option lets us specify the **Privacy Enhanced Mail** (**PEM**) file that was provided by AWS for the key pair. If on Windows using PuTTY, you'd instead tell it which **PuTTY Private Key** (**PPK**) file to use, and the connection parameters will otherwise be similar to this.\n\nThis lands us at the command-line prompt of the EC2 instance. We see that it is Ubuntu 20.04, and the hostname is set to `notes-public`, as reflected in Command Prompt and the output of the `hostname` command. This means that our initialization script ran because the hostname was the last configuration task it performed.\n\n### Handling the AWS EC2 key-pair file\n\nEarlier, we said to safely store the key-pair file somewhere on your computer.\u00a0 In the previous section, we showed how to use the PEM file with SSH to log in to the EC2 instance. Namely, we use the PEM file like so:\n\n```", "```js\\1\n\nAs the command name implies, this adds the authentication file to SSH. This has to be rerun on every reboot of the computer, but it conveniently lets us access EC2 instances without remembering to specify this option.\n\n### Testing the initial Docker Swarm\n\nWe have an EC2 instance and it should already be\u00a0configured with Docker, and we can easily verify that this is the case as follows:\n\n```", "```js\\1\n\nThe `docker info` command, as the name implies, prints out a lot of information about the current Docker instance. In this case, the output includes verification that it is in Docker Swarm mode and that this is a Docker Swarm manager instance.\n\nLet's try a couple of swarm commands, as follows:\n\n```", "```js\\1\n\nWe started one service using the `nginx` image. We said to deploy one replica and to expose port `80`. We chose the `nginx` image because it has a simple default HTML file that we can easily view, as illustrated in the following screenshot:\n\n![](img/28a6d679-cfef-4fbf-a8b4-6d7413d2e3b8.png)\n\nSimply paste the IP address of the EC2 instance into the browser location bar, and we're greeted with that default HTML.\n\nWe also see by using `docker node ls` and `docker service ps` that there is one instance of the service. Since this is a swarm, let's increase the number of `nginx` instances, as follows:\n\n```", "```js\\1\n\nThis verifies that the `nginx` service with three replicas is actually three `nginx` containers.\n\nIn this section, we were able to launch an EC2 instance and set up a single-node Docker swarm in which we launched a service, which gave us the opportunity to familiarize ourselves with what this can do.\n\nWhile we're here, there is another thing to learn\u2014namely, how to set up the remote control of Docker hosts.\n\n## Setting up remote control access to a Docker Swarm hosted on EC2\n\nA feature that's not well documented in Docker is the ability to control Docker nodes remotely. This will let us, from our laptop, run Docker commands on a server. By extension, this means that we will be able to manage the Docker Swarm from our laptop.\n\nOne method for remotely controlling a Docker instance is to expose the Docker **Transmission Control Protocol** (**TCP**) port. Be aware that miscreants are known to scan an internet infrastructure for Docker ports to hijack. The following technique does not expose the Docker port but instead uses SSH.\n\nThe following setup is for Linux and macOS, relying on features of SSH. To do this on Windows would rely on installing OpenSSH. From October 2018, OpenSSH became available for Windows, and the following commands may work in PowerShell (failing that, you can run these commands from a Multipass or **Windows Subsystem for Linux**\u00a0(**WSL**) 2\u00a0instance on Windows):\n\n```", "```js\\1\n\nWe discussed this command earlier, noting that it lets us log in to EC2 instances without having to use the `-i`\u00a0option to specify the PEM file.\u00a0 This is more than a simple convenience when it comes to remotely accessing Docker hosts. The following steps are dependent on having added the PEM file to SSH, as shown here.\u00a0\n\nTo verify you've done this correctly, use this command:\n\n```", "```js\\1\n\nThe `DOCKER_HOST` environment variable enables the remote control of Docker hosts. It relies on a passwordless SSH login to the remote host. Once you have that, it's simply a matter of setting the environment variable and you've got remote control of the Docker host, and in this case, because the host is a swarm manager, a remote swarm.\n\nBut this gets even better by using the Docker context feature. A *context* is a configuration required to access a remote node or swarm. Have a look at the following code snippet:\n\n```", "```js\\1\n\nWe create a context using `docker context create`, specifying the same SSH URL\u00a0we used in the `DOCKER_HOST` variable. We can then use it either with the `--context` option or by using `docker context use` to switch between contexts.\n\nWith this feature, we can easily maintain configurations for multiple remote servers and switch between them with a simple command.\n\nFor example, the Docker instance on our laptop is the *default* context. Therefore, we might find ourselves doing this:\n\n```", "```js\\1\n\nThe effect of changing the name of one of the Terraform files is that Terraform will not scan those files for objects to deploy. Therefore, when Terraform maps out the state we want Terraform to deploy, it will notice that the deployed EC2 instance and security group are not listed in the local files, and it will, therefore, destroy those objects. In other words, this lets us undeploy some infrastructure with very little fuss.\u00a0\n\nThis tactic can be useful for minimizing costs by turning off unneeded facilities. You can easily redeploy the EC2 instances by renaming the file back to `ec2-public.tf`\u00a0and rerunning `terraform apply`.\n\nIn this section, we familiarized ourselves with Docker Swarm by deploying a single-node swarm on an EC2 instance on AWS. We first added suitable declarations to our Terraform files. We then deployed the EC2 instance on AWS. Following deployment, we set about verifying that, indeed, Docker Swarm was already installed and initialized on the server and that we could easily deploy Docker services on the swarm. We then learned how to set up remote control of the swarm from our laptop.\n\nTaken together, this proved that we can easily deploy Docker-based services to EC2 instances on AWS. In the next section, let's continue preparing for a production-ready deployment by setting up a build process to push Docker images to image repositories.\n\n# Setting up ECR repositories for Notes Docker images\n\nWe have created Docker images to encapsulate the services making up the Notes application. So far, we've used those images to instantiate Docker containers on our laptop. To deploy containers on the AWS infrastructure will require the images to be hosted in a Docker image repository.\n\nThis requires a build procedure by which the `svc-notes` and `svc-userauth` images are correctly pushed to the container repository on the AWS infrastructure. We will go over the commands required and create a few shell scripts to record those commands.\u00a0\n\nA site such as Docker Hub is what's known as a Docker Registry. Registries are web services that store Docker images by hosting Docker image repositories. When we used the\u00a0`redis`\u00a0or\u00a0`mysql/mysql-server`\u00a0images earlier, we were using Docker image repositories located on the Docker Hub Registry.\u00a0\n\nThe AWS team offers a Docker image registry, ECR. An ECR instance is available for each account in each AWS region. All we have to do is log in to the registry, create repositories, and push images to the repositories.\n\nIt is extremely important to run commands in this section in the default Docker context on your laptop. The reason is that Docker builds must not happen on the Swarm host but on some other host, such as your laptop.\n\nBecause it is important to not run Docker build commands on the Swarm infrastructure, execute this command:\n\n```", "```js\\1\n\nThis command, and others, are available in the ECR dashboard. If you navigate to that dashboard and then create a repository there, a button labeled\u00a0View Push Command\u00a0is available. This and other useful commands are listed there,\u00a0but we have substituted a few variable names to make this configurable.\n\nIf you are instead using Windows PowerShell, AWS recommends the following:\n\n```", "```js\\1\n\nThis is the same command as is used for Unix-like systems, but with Windows-style references to environment variables.\u00a0\u00a0\n\nYou may wish to explore the `cross-var` package, since it can convert Unix-style environment variable references to Windows. For the documentation, refer to\u00a0[https://www.npmjs.com/package/cross-var](https://www.npmjs.com/package/cross-var).\n\nSeveral environment variables are being used, but just what are those variables being used and how do we set them?\n\n## Using environment variables for AWS CLI commands\n\nLook carefully and you will see that some environment variables are being used. The AWS CLI commands know about those environment variables and will use them instead of command-line options. The environment variables we're using are the following:\n\n*   `AWS_PROFILE`: The AWS profile to use with this project.\u00a0\n*   `AWS_REGION`: The AWS region to deploy the project to.\n*   `AWS_USER`: The numeric user ID for the account being used. This ID is available on the IAM dashboard page for the account.\n\nThe AWS CLI recognizes some of these environment variables, and others. For further details, refer to\u00a0[https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html).\n\nThe AWS command-line tools will use those environment variables in place of the command-line options. Earlier, we discussed using the\u00a0`AWS_PROFILE`\u00a0variable instead of the\u00a0`--profile`\u00a0option. The same holds true for other command-line options.\n\nThis means that we need an easy way to set those variables. These Bash commands can be recorded in a shell script like this, which you could store as `env-us-west-2`:\n\n```", "```js\\1\n\nFor other command environments, again transliterate appropriately. For example, in Windows and in PowerShell, the variables can be set with these commands:\n\n```", "```js\\1\n\nThis handles running\u00a0`docker build`\u00a0commands for both the Notes and user authentication services. It is expected to be executed in the `ecr` directory and takes care of executing commands in both the `notes` and `users` directories.\n\nLet's now create and delete a pair of registries to hold our images. We have two images to upload to the ECR, and therefore we create two registries.\u00a0\n\nCreate a file named `create.sh` containing the following:\n\n```", "```js\\1\n\nBetween these scripts, we can create and delete the ECR repositories for our Docker images. These scripts are directly usable on Windows; simply change the filenames to `create.ps1` and `delete.ps1`.\n\nIn `aws ecr delete-repository`, the `--force` option means to delete the repositories even if they contain images.\n\nWith the scripts we've written so far, they are executed in the following\u00a0order:\n\n```", "```js\\1\n\nThe\u00a0`docker tag`\u00a0command we have here takes\u00a0`svc-notes:latest`, or\u00a0`svc-userauth:latest`, and adds what's called a target image\u00a0to the local image storage area. The target image name we've used is the same as what will be stored in the ECR repository.\n\nFor Windows, you should create a file named `tag.ps1` using the same commands, but with Windows-style environment variable references.\n\nThen, create a file named `push.sh` containing the following:\n\n```", "```js\\1\n\nThis builds the Docker images. When we run\u00a0`docker build`, it stores the built image in an area on our laptop where Docker maintains images. We can inspect that area using the\u00a0`docker images`\u00a0command, like this:\n\n```", "```js\\1\n\nSince the images are rather large, it will take a long time to upload them to the AWS ECR. We should add a task to the backlog to explore ways to trim Docker image sizes. In any case, expect this to take a while.\u00a0\n\nAfter a period of time, the images will be uploaded to the ECR repositories, and you can inspect the results on the ECR dashboard.\n\nOnce the Docker images are pushed to the AWS ECR repository, we no longer need to stay with the default Docker context. You will be free to run the following command at any time:\n\n```", "```js\\1\n\nFor the Compose file we started with, version `'3'` was adequate, but to accomplish the tasks in this chapter\u00a0the higher version number is required, to enable newer features.\n\nFortunately, most of this is straightforward and will require very little code.\n\n*Deployment parameters*: These are expressed in the\u00a0`deploy`\u00a0tag, which covers things such as the number of replicas, and memory or CPU requirements. For documentation, refer to\u00a0[https://docs.docker.com/compose/compose-file/#deploy](https://docs.docker.com/compose/compose-file/#deploy).\n\nFor the deployment parameters, simply add a `deploy` tag to each service. Most of the options for this tag have perfectly reasonable defaults. To start with, let's add this to every service, as follows:\n\n```", "```js\\1\n\nIf we use this with `docker-compose`, it will perform the build in the named directories, and then tag the resulting image with the tag in the `image` field. In this case, the `deploy` tag will be ignored as well. However, if we use this with `docker stack deploy`, the `build` tag will be ignored, and the images will be downloaded from the repositories listed in the `image` tag. In this case, the `deploy` tag will be used.\n\nFor documentation on the `build` tag, refer to\u00a0[https://docs.docker.com/compose/compose-file/#build](https://docs.docker.com/compose/compose-file/#build). For documentation on the `image` tag, refer to\u00a0[https://docs.docker.com/compose/compose-file/#image](https://docs.docker.com/compose/compose-file/#image)[.](https://docs.docker.com/compose/compose-file/#build)\n\nWhen running the compose file on our laptop, we used `bridge` networking. This works fine for a single host, but with swarm mode, we need another network mode that handles multi-host deployments. The Docker documentation clearly says to use the `overlay` driver in swarm mode, and the `bridge` driver for a single-host deployment.\u00a0\n\n*Virtual networking for containers*: Since `bridge` networking is designed for a single-host deployment, we must use `overlay` networking in swarm mode. For documentation, refer to\u00a0[https://docs.docker.com/compose/compose-file/#network-configuration-reference](https://docs.docker.com/compose/compose-file/#network-configuration-reference).\n\nTo use overlay networking, change the `networks` tag to the following:\n\n```", "```js\\1\n\nThe `placement` tag governs where the containers are deployed. Rather than Docker evenly distributing the containers, we can influence the placement with the fields in this tag. In this case, we have two examples, such as deploying a container to a specific node based on the hostname or selecting a node based on the labels attached to the node.\n\nTo set a label on a Docker swarm node, we run the following command:\n\n```", "```js\\1\n\nThis gives us three labels to assign to our EC2 instances: `db`, `svc`, and `public`. These\u00a0constraints will cause the databases to be placed on nodes where the\u00a0`type` label is\u00a0`db`, the user authentication service is on the node of type\u00a0`svc`, the Notes service is on the\u00a0`public` node, and the Redis service is on any node that is not the\u00a0`public` node.\n\nThe reasoning stems from the security model we designed. The containers deployed on the private network should be more secure behind more layers of protection. This placement leaves the Notes container as the only one on the public EC2 instance. The other containers are split between the\u00a0`db` and\u00a0`svc` nodes. We'll see later how these labels will be assigned to the EC2 instances we'll create.\n\n### Configuring secrets in Docker Swarm\n\nWith Notes, as is true for many kinds of applications, there are some secrets we must protect. Primarily, this is the Twitter authentication tokens, and we've claimed it could be a company-ending event if those tokens were to leak to the public. Maybe that's overstating the danger, but leaked credentials could be bad. Therefore, we must take measures to ensure that those secrets do not get committed to a source repository as part of any source code, nor should they be recorded in any other file.\n\nFor example, the Terraform state file records all information about the infrastructure, and the Terraform team makes no effort to detect any secrets and suppress recording them. It's up to us to make sure the Terraform state file does not get committed to source code control as a result.\n\nDocker Swarm supports a very interesting method for securely storing secrets and for making them available in a secure manner in containers.\n\nThe process starts with the following command:\n\n```", "```js\\1\n\nThis lets Docker know that this stack requires the value of those two secrets.\u00a0\n\nThe declaration for `svc-notes` also needs the following command:\n\n```", "```js\\1\n\nThen, we'll find the code corresponding to these environment variables further down the file. We should rewrite that section as follows:\n\n```", "```js\\1\n\nIn `docker-local/docker-compose.yml`, we used the named volumes, `db-userauth-data` and `db-notes-data`. The top-level `volumes` tag is required when doing this. In `docker-swarm/docker-compose.yml`, we've commented all of that out. Instead, we are using a\u00a0`bind` mount, to mount specific host directories in the `/var/lib/mysql` directory of each database.\n\nTherefore, the database data directories will be in `/data/users` and `/data/notes`, respectively.\n\nThis result is fairly good, in that we can destroy and recreate the database containers at will and the data directories will persist. However, this is only as persistent as the EC2 instance this is deployed to. The data directories will vaporize as soon as we execute `terraform destroy`.\n\nThat's obviously not good enough for a production deployment, but it is good enough for a test deployment such as this.\u00a0\n\nIt is preferable to use a volume instead of the `bind` mount we just implemented. Docker volumes have a number of advantages, but to make good use of a volume requires finding the right volume driver for your needs. Two examples are as follows:\n\n1.  In the Docker documentation, at\u00a0[https://docs.docker.com/storage/volumes/](https://docs.docker.com/storage/volumes/), there is an example of mounting a **Network File System** (**NFS**) volume in a Docker container. AWS offers an NFS service\u2014the **Elastic Filesystem** (**EFS**) service\u2014that could be used, but this may not be the best choice for a database container.\n2.  The REX-Ray project ([https://github.com/rexray/rexray](https://github.com/rexray/rexray)) aims to advance the state of the art for persistent data storage in various containerization systems, including Docker.\n\nAnother option is to completely skip running our own database containers and instead use the **Relational Database Service** (**RDS**). RDS is an AWS service offering several **Structured Query Language** (**SQL**) database solutions, including MySQL. It offers a lot of flexibility and scalability, at a price. To use this, you would eliminate the `db-notes` and `db-userauth` containers, provision RDS instances, and then update the `SEQUELIZE_CONNECT` configuration in `svc-notes` and `svc-userauth` to use the database host, username, and password you configured in the RDS instances.\n\nFor our current requirements, this setup, with a `bind` mount to a directory on the EC2 host, will suffice. These other options are here for your further exploration.\n\nIn this section, we converted our Docker compose file to be useful as a stack file. While doing this, we discussed the need to influence which swarm host has which containers. The most critical thing is ensuring that the database containers are deployed to a host where we can easily persist the data\u2014for example, by running a database backup every so often to external storage. We also discussed storing\u00a0secrets\u00a0in a secure manner so that they may be used safely\u00a0by the containers.\n\nAt this point, we cannot test the stack file that we've created because we do not have a suitable swarm to deploy to.\u00a0Our next step is writing the Terraform configuration to provision the EC2 instances. That will give us the Docker swarm that lets us test the stack file.\n\n# Provisioning EC2 instances for a full Docker swarm\n\nSo far in this chapter, we have used Terraform to create the required infrastructure on AWS, and then we set up a single-node Docker swarm on an EC2 instance to learn about Docker Swarm. After that, we pushed the Docker images to ECR, and we have set up a Docker stack file for deployment to a swarm. We are ready to set up the EC2 instances required for deploying a full swarm.\n\nDocker Swarm is able to handle Docker deployments to large numbers of host systems. Of course, the Notes application only has delusions of grandeur and doesn't need that many hosts. We'll be able to do everything with three or four EC2 instances. We have declared one so far, and will declare two more that will live on the private subnet. But from this humble beginning, it would be easy to expand to more hosts.\n\nOur goal in this section is to create an infrastructure for deploying Notes on EC2 using Docker Swarm. This will include the following:\n\n*   Configuring additional EC2 instances on the private subnet, installing Docker on those instances, and joining them together in a multi-host Docker Swarm\n*   Creating semi-automated scripting, thereby making it easy to deploy and configure the EC2 instances for the swarm\n*   Using an `nginx` container on the public EC2 instance as a proxy in front of the Notes container\n\nThat's quite a lot of things to take care of, so let's get started.\n\n## Configuring EC2 instances and connecting to the swarm\n\nWe have one EC2 instance declared for the public subnet, and it is necessary to add two more for the private subnet. The security model we discussed earlier focused on keeping as much as possible in a private secure network infrastructure. On AWS, that means putting as much as possible on the private subnet.\n\nEarlier, you may have renamed `ec2-public.tf` to `ec2-public.tf-disable`. If so, you should now change back the filename to `ec2-public.tf`. Remember that this tactic is useful for minimizing AWS resource usage when it is not needed.\n\nCreate a new file in the `terraform-swarm` directory named `ec2-private.tf`, as follows:\n\n```", "```js\\1\n\nThis is the security group for these EC2 instances. It allows any traffic from inside the VPC to enter the EC2 instances. This is the sort of security group we'd create when in a hurry and\u00a0should tighten up the ingress rules, since this is very lax.\n\nLikewise, the `ec2-public-sg` security group needs to be equally lax. We'll find that there is a long list of IP ports used by Docker Swarm and that the swarm will fail to operate unless those ports can communicate. For our immediate purposes, the easiest option is to allow any traffic, and we'll leave a note in the backlog to address this issue in [Chapter 14](4cccad1e-fe7e-495a-9e90-8818820b890a.xhtml),\u00a0*Security in Node.js Applications*.\n\nIn `ec2-public.tf`, edit the `ec2-public-sg` security group to be the following:\n\n```", "```js\\1\n\nThis outputs the useful attributes of the EC2 instances.\n\nIn this section, we declared EC2 instances for deployment on the private subnet. Each will have Docker initialized.\u00a0However, we still need to do what we can to automate the setup of the swarm.\n\n## Implementing semi-automatic initialization of the Docker Swarm\n\nIdeally, when we run `terraform apply`, the infrastructure is automatically set up and ready to go. Automated setup reduces the overhead of running and maintaining the AWS infrastructure. We'll get as close to that goal as possible.\n\nFor this purpose, let's revisit the declaration of `aws_instance.public` in `ec2-public.tf`. Let's rewrite it as follows:\n\n```", "```js\\1\n\nThis generates a shell script that will be used to initialize the swarm. Because the setup relies on executing commands on the other EC2 instances, the PEM file for the AWS key pair must be present on the `notes-public` instance. However, it is not possible to send the key-pair file to the `notes-public` instance when running `terraform apply`. Therefore, we use the pattern of generating a shell script, which will be run later.\n\nThe pattern being followed is\u00a0shown in the following code snippet:\n\n```", "```js\\1\n\nThe first line uses SSH to execute the\u00a0`swarm join`\u00a0command on the EC2 instance. For this to work, we need to supply the AWS key pair, which must be specified on the command file so that it becomes the `PEM` variable. The second line adds the\u00a0`type` label with the named value to the named swarm node.\n\nWhat is the `$join` variable? It has the output of running `docker swarm join-token`, so let's take a look at what it is.\n\nDocker uses a\u00a0swarm join token to facilitate connecting Docker hosts as a node in a swarm. The token contains cryptographically signed information that authenticates the attempt to join the swarm. We get the token by running the following\u00a0command:\n\n```", "```js\\1\n\nThis deploys the EC2 instances on AWS. Make sure to record all the output parameters. We're especially interested in the domain names and IP addresses for the three EC2 instances.\n\nAs before, the `notes-public` instance should have a Docker swarm initialized. We have added two more instances, `notes-private-db1` and `notes-private-svc1`. Both will have Docker installed, but they are not joined to the swarm. Instead, we need to run the generated shell script for them to become nodes in the swarm, as follows:\n\n```", "```js\\1\n\nIn this case, we are testing the private EC2 instances from a shell running on the public EC2 instance. That means we must use the private IP addresses printed when we ran Terraform. This command verifies SSH connectivity to an EC2 instance and verifies its ability to download and execute a Docker image.\n\nNext, we can run `swarm-setup.sh`. On the command line, we must give the filename for the PEM file as the first argument, as follows:\n\n```", "```js\\1\n\nIndeed, these systems are now part of the cluster.\u00a0\n\nThe swarm is ready to go, and we no longer need to be logged in to `notes-public`. Exiting back to our laptop, we can create the Docker context to control the swarm remotely, as follows:\n\n```", "```js\\1\n\nFrom our laptop, we can query the state of the remote swarm that's hosted on AWS. Of course, this isn't limited to querying the state; we can run any other Docker command.\n\nWe also need to run the following commands, now that the swarm is set up:\n\n```", "```js\\1\n\nThis script executes the same commands we just went over to prepare the swarm on the EC2 hosts.\u00a0It requires the environment variables to be set, as follows:\n\n*   `AWS_KEY_PAIR`: The filename for the PEM file\n*   `NOTES_PUBLIC_IP`: The IP address of the `notes-public` EC2 instance\n*   `TWITTER_CONSUMER_KEY`, `TWITTER_CONSUMER_SECRET`: The access tokens for Twitter authentication\n\nIn this section, we have deployed more EC2 instances and set up the Docker swarm. While the process was not completely automated, it's very close. All that's required, after using Terraform to deploy the infrastructure, is to execute a couple of commands to get logged in to `notes-public` where we run a script, and then go back to our laptop to set up remote access.\n\nWe have set up the EC2 instances and verified we have a working swarm. We still have the outstanding issue of verifying the Docker stack file created in the previous section. To do so, our next step is to deploy the Notes app on the swarm.\n\n# Deploying the Notes stack file to the swarm\n\nWe have prepared all the elements required to set up a Docker Swarm on the AWS EC2 infrastructure, we have run the scripts required to set up that infrastructure, and we have created the stack file required to deploy Notes to the swarm.\n\nWhat's required next is to run `docker stack deploy` from our laptop, to deploy Notes on the swarm. This will give us the chance to test the stack file created earlier. You should still have the Docker context configured for the remote server, making it possible to remotely deploy the stack. However, there are four things to handle first, as follows:\n\n1.  Install the secrets in the newly deployed swarm.\n2.  Update the `svc-notes` environment configuration for the IP address of `notes-public`.\n3.  Update the Twitter application for the IP address of `notes-public`.\n4.  Log in to the ECR instance.\n\nLet's take care of those things and then deploy the Notes stack.\n\n## Preparing to deploy the Notes stack to the swarm\n\nWe are ready to deploy the Notes stack to the swarm that we've launched. However, we have realized that we have a couple of tasks to take care of.\n\nThe environment variables for `svc-notes` configuration require a little adjustment. Have a look at the following code block:\n\n```", "```js\\1\n\nThis has to be rerun every so often since the tokens that are downloaded time out after a few hours.\n\nWe only need to run `login.sh`, and none of the other scripts in the `ecr` directory.\n\nIn this section, we prepared to run the deployment. We should now be ready to deploy Notes to the swarm, so let's do it.\n\n## Deploying the Notes stack to the swarm\n\nWe just did the final preparation for deploying the Notes stack to the swarm. Take a deep breath, yell out *Smoke Test*, and type the following command:\n\n```", "```js\\1\n\nThe `service ls` command lists the services, with a high-level overview. Remember that the service is not the running container and, instead, the services are declared by entries in the `services` tag in the stack file. In our case, we declared one replica for each service, but we could have given a different amount. If so, the swarm will attempt to distribute that number of containers across the nodes in the swarm.\n\nNotice that the pattern for service names is the name of the stack that was given in the `docker stack deploy` command, followed by the service name listed in the stack file. When running that command, we named the stack\u00a0`notes`; so, the services are `notes_db-notes`, `notes_svc-userauth`, `notes_redis`, and so on.\n\nThe `service ps` command lists information about the tasks deployed for the service. Remember that a task is essentially the same as a running container. We see here that one instance of the `svc-notes` container has been deployed, as expected, on the `notes-public` host.\n\nSometimes, the `notes_svc-notes` service doesn't launch, and instead, we'll see the following\u00a0message:\n\n```", "```js\\1\n\nNotice that the\u00a0`Labels` entry is empty. In such a case, you can add the label by running this command:\n\n```", "```js\\1\n\nIt would appear that this provides a small window of opportunity to allow the swarm to establish itself.\n\n### Diagnosing a failure to launch the database services\n\nAnother possible deployment problem is that the database services might fail to launch, and the `notes-public-db1` node might become\u00a0`Unavailable`. Refer back to the `docker node ls` output and you will see a column marked `Status`. Normally, this column says\u00a0`Reachable`, meaning that the swarm can reach and communicate with the swarm agent on that node. But with the deployment as it stands, this node might instead show an\u00a0`Unavailable` status, and in the `docker service ls` output, the database services might never show as having deployed.\n\nWith remote access from our laptop, we can run the following command:\n\n```", "```js\\1\n\nThat gets us access to the public EC2 instance. From there, we can try to ping the `notes-private-db1` instance, as follows:\n\n```", "```js\\1\n\nThis changes the instance type from `t2.micro` to `t2.medium`, or even `t2.large`, thereby giving the server more memory.\n\nTo implement this change, run `terraform apply` to update the configuration. If the swarm does not automatically correct itself, then you may need to run `terraform destroy`\u00a0and then run through the setup again, starting with `terraform apply`.\u00a0\n\nOnce the `notes-private-db1` instance has sufficient memory, the databases should successfully deploy.\n\nIn this section, we deployed the Notes application stack to the swarm cluster on AWS. We also talked a little about how to verify the fact that the stack deployed correctly, and how to handle some common problems.\n\nNext, we have to test the deployed Notes stack to verify that it works on AWS.\n\n## Testing the deployed Notes application\n\nHaving set up everything required to deploy Notes to AWS using Docker Swarm, we have done so. That means our next step is to put Notes through its paces. We've done enough ad hoc testing on our laptop to have confidence it works, but the Docker swarm deployment might show up some issues.\n\nIn fact, the deployment we just made very likely has one or two problems. We can learn a lot about AWS and Docker Swarm by diagnosing those problems together.\n\nThe first test is obviously to open the Notes application in the browser. In the outputs from running `terraform apply` was a value labeled\u00a0`ec2-public-dns`. This is the domain name for the `notes-public` EC2 instance. If we simply paste that domain name into our browser, the Notes application should appear.\n\nHowever, we cannot do anything because there are no user IDs available to log in with.\n\n### Logging in with a regular account on Notes\n\nObviously, in order to test Notes, we must log in and add some notes, make some comments, and so forth. It will be instructive to log in to the user authentication service and use `cli.mjs` to add a user ID.\n\nThe user authentication service is on one of the private EC2 instances, and its port is purposely not exposed to the internet. We could change the configuration to expose its port and then run `cli.mjs` from our laptop, but that would be a security problem and we need to learn how to access the running containers anyway.\n\nWe can find out which node the service is deployed on by using the following command:\n\n```", "```js\\1\n\nWe SSHd to the `notes-public` server and, from there, SSHd to the `notes-private-svc1` server. On that server, we ran `docker ps` to find out the name of the running container. Notice that Docker generated a container name that includes a coded string, called a\u00a0*nonce*, that guarantees the container name is unique. With that container name, we ran `docker exec -it ... bash`\u00a0to get a root shell inside the container.\n\nOnce there, we can run the following command:\n\n```", "```js\\1\n\nFrom there, we can explore the database and see that, indeed, Ashildr's user ID exists.\n\nWith this user ID set up, we can now use our browser to visit the Notes application and log in with that user ID.\n\n### Diagnosing an inability to log in with Twitter credentials\n\nThe next step will be to test logging in with Twitter credentials. Remember that earlier, we said to ensure that the\u00a0`TWITTER_CALLBACK_HOST` variable has the domain name of the EC2 instance, and likewise that the Twitter application configuration does as well.\u00a0\n\nEven with those settings in place, we might run into a problem. Instead of logging in, we might get an error page with a stack trace, starting with the message:\u00a0`Failed to obtain request token`.\u00a0\n\nThere are a number of possible issues that can cause this error. For example, the error can occur if the Twitter authentication tokens are not deployed. However, if you followed the directions correctly, they will be deployed correctly.\n\nIn `notes/appsupport.mjs`, there is a function, `basicErrorHandler`, which will be invoked by this error. In that function, add this line of code:\n\n```", "```js\\1\n\nHowever, if we attempt this inside the `svc-notes` container, this might fail, as illustrated in the following code snippet:\n\n```", "```js\\1\n\nThese two DNS servers are operated by Google, and indeed this solves the problem. Once this change has been made, you should be able to log in to Notes using Twitter credentials.\n\nIn this section, we tested the Notes application and discussed how to diagnose and remedy a couple of common problems. While doing so, we learned how to navigate our way around the EC2 instances and the Docker Swarm.\n\nLet's now see what happens if we change the number of instances for our services.\n\n## Scaling the Notes instances\n\nBy now, we have deployed the Notes stack to the cluster on our EC2 instances. We have tested everything and know that we have a correctly functioning system deployed on AWS. Our next task is to increase the number of instances and see what happens.\n\nTo increase the instances for `svc-notes`, edit `compose-swarm/docker-compose.yml` as follows:\n\n```", "```js\\1\n\nEarlier, this command described its actions with the word\u00a0*Creating*, and this time it used the word\u00a0*Updating*. This means that the services are being updated with whatever new settings are in the stack file.\n\nAfter a few minutes, you may see this:\n\n```", "```js\\1\n\nAs we saw earlier, this command lists to which swarm nodes the service has been deployed. In this case, we'll see that both instances are on `notes-public`, due to the placement constraints.\n\nAnother useful command is the following:\n\n```"]