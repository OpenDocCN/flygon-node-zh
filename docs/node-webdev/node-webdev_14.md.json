["```js\\1\n\nThe\u00a0`docker run`\u00a0command downloads a Docker image, named on the command line, initializes a Docker container from that image, and then runs that container. In this case, the image, named\u00a0`hello-world`, was not present on the local computer and had to be downloaded and initialized. Once that was done, the\u00a0`hello-world`\u00a0container was executed and it printed out these instructions.\n\nThe `docker run hello-world` command is a quick way to verify that Docker is installed correctly.\n\nLet's follow the suggestion and start an Ubuntu container:\n\n```", "```js\\1\n\nThe `clever_napier`\u00a0name\u00a0is the container name automatically generated by Docker. While the image\u00a0name was\u00a0`hello-world`, that was not the container name. Docker generated the container name so that you have a more user-friendly identifier for the containers than the hex ID shown in the `CONTAINER ID` column:\n\n```", "```js\\1\n\nThe `docker pull` command retrieves an image from a Docker repository and is conceptually similar to the `git pull` command, which retrieves changes from a `git` repository.\n\nThis downloaded four image layers in total because this image is built on top of three other images. We'll see later how that works when we learn how to build a Dockerfile.\u00a0\n\nWe can query which images are stored on our laptop with the following command:\n\n```", "```js\\1\n\nNotice that the actual `delete` operation works with the SHA256 image identifier.\n\nA container can be launched\u00a0with the image, as follows:\n\n```", "```js\\1\n\nThe\u00a0**`docker exec`**\u00a0command lets you run programs inside the container. The\u00a0`-it`\u00a0option says the command is run interactively on an assigned terminal. In this case, we used the\u00a0`mysql`\u00a0command to run the MySQL client so that we could interact with the database. Substitute\u00a0`bash`\u00a0for\u00a0`mysql`, and you will land in an interactive\u00a0`bash`\u00a0command shell.\n\nThis\u00a0`mysql`\u00a0command instance is running inside the container. The container is configured by default to not expose any external ports, and it has a default\u00a0`my.cnf`\u00a0file.\u00a0\n\nDocker containers are meant to be ephemeral, created and destroyed as needed, while databases are meant to be permanent, with lifetimes\u00a0sometimes\u00a0measured in decades. A very important discussion on this point and how it applies to database containers is presented in the next section.\n\nIt is cool that we can easily install and launch a MySQL instance. However, there are several considerations to be made:\n\n*   Access to the database from other software, specifically from another container\n*   Storing the database files outside the container for a longer lifespan\n*   Custom configuration, because database admins love to tweak the settings\n*   We need a path to connect the MySQL container to the AuthNet network that we'll be creating\n\nBefore proceeding, let's clean up. In a terminal window, type the following:\n\n```", "```js\\1\n\nWe'll be adding more scripts to this file. The\u00a0`build-authnet`\u00a0command builds a virtual network using the\u00a0`bridge`\u00a0driver, as we just discussed. The name for this network is\u00a0`authnet`.\n\nHaving created\u00a0`authnet`,\u00a0we can attach containers to it so that the containers can communicate with one another.\n\nOur goal for the Notes application stack is to use private networking between containers to implement a security firewall around the containers. The containers will be able to communicate with one another, but the private network is not reachable by any other software and is, therefore, more or less safe from intrusion.\n\nType the following command:\n\n```", "```js\\1\n\nAt the moment, this won't show any containers attached to\u00a0`authnet`. The output shows the network name, the IP range of this network, the default gateway, and other useful network configuration information. Since nothing is connected to the network, let's get started with building the required containers:\n\n```", "```js\\1\n\nThis does several useful things all at once. It initializes an empty database configured with the named users and passwords, it mounts a host directory as the MySQL data directory, it attaches the new container to\u00a0`authnet`, and it exposes the MySQL port to connections from outside the container.\n\nThe\u00a0`docker run`\u00a0command is only run the first time the container is started. It combines building the container by running it for the first time. With the MySQL container, its first run is when the database is initialized. The options that are passed to this\u00a0`docker run`\u00a0command are meant to tailor the database initialization.\n\nThe\u00a0`--env`\u00a0option sets environment variables inside the container. The scripts driving the MySQL container look to these environment variables to determine the user IDs, passwords, and database to create.\n\nIn this case, we configured a password for the\u00a0`root`\u00a0user, and we configured a second user\u2014`userauth`\u2014with a matching password and database name.\n\nThere are many more environment variables available.\n\nThe official MySQL Docker documentation provides more information on configuring a MySQL Docker container ([https://dev.mysql.com/doc/refman/8.0/en/docker-mysql-more-topics.html](https://dev.mysql.com/doc/refman/8.0/en/docker-mysql-more-topics.html)).\n\nThe MySQL server recognizes an additional set of environment variables ([https://dev.mysql.com/doc/refman/8.0/en/environment-variables.html](https://dev.mysql.com/doc/refman/8.0/en/environment-variables.html)).\n\nThe MySQL server recognizes a long list of configuration options that can be set on the command line or in the MySQL configuration file ([https://dev.mysql.com/doc/refman/8.0/en/server-option-variable-reference.html](https://dev.mysql.com/doc/refman/8.0/en/server-option-variable-reference.html)).\n\nThe\u00a0`--network`\u00a0option attaches the container to the\u00a0`authnet`\u00a0network.\n\nThe\u00a0`-p`\u00a0option exposes a TCP port from inside the container so that it is visible outside the container. By default, containers do not expose any TCP ports. This means we can be very selective about what to expose, limiting the attack surface for any miscreants seeking to gain illicit access to the container.\n\nThe\u00a0`--mount`\u00a0option is meant to replace the older\u00a0`--volume`\u00a0option. It is a powerful tool for attaching external data storage to a container. In this case, we are attaching a host directory,\u00a0`userauth-data`, to the\u00a0`/var/lib/mysql`\u00a0directory inside the container. This ensures that the database is not inside the container, and that it will last beyond the lifetime of the container. For example, while creating this example, we deleted this container several times to fine-tune the command line, and it kept using the same data directory.\n\nWe should also mention that the\u00a0`--mount`\u00a0option requires the\u00a0`src=`\u00a0option be a full pathname to the file or directory that is mounted. We are using\u00a0``pwd``\u00a0to determine the full path to the file.\u00a0However, this is, of course, specific to Unix-like OSes. If you are on Windows, the command should be run in PowerShell and you can use the\u00a0`$PSScriptRoot`\u00a0variable.\u00a0Alternatively, you can hardcode an absolute pathname.\n\nIt is possible to inject a custom\u00a0`my.cnf`\u00a0file into the container by adding this option to the\u00a0`docker run`\u00a0command:\n\n```", "```js\\1\n\nSo far, we have talked about the options for the `docker run` command. Those options configure the characteristics of the container. Next on the command line is the image name\u2014in this case,\u00a0`mysql/mysql-server:8.0`. Any command-line tokens appearing after the image name are passed into the container. In this case, they are interpreted as arguments to the MySQL server, meaning we can configure this server using any of the extensive sets of command-line options it supports. While we can mount a `my.cnf` file in the container, it is possible to achieve most configuration settings this way.\n\nThe first of these options,\u00a0`--bind_address`, tells the server to listen for connections from any IP address.\n\nThe second, `--socket=/tmp/mysql.sock`, serves two purposes. One is security, to ensure that the MySQL Unix domain socket is accessible only from inside the container. By default, the scripts inside the MySQL container put this socket in the `/var/lib/mysql` directory, and when we attach the data directory, the socket is suddenly visible from outside the container.\n\nOn Windows, if this socket is in\u00a0`/var/lib/mysql`, when we attach a data directory to the container, that would put the socket in a Windows directory. Since Windows does not support Unix domain sockets, the MySQL container will mysteriously fail to start and give a misleadingly obtuse error message. The `--socket` option ensures that the socket is instead on a filesystem that supports Unix domain sockets, avoiding the possibility of this failure.\u00a0\n\nWhen experimenting with different options, it is important to delete the mounted data directory each time you recreate the container to try a new setting. If the MySQL container sees a populated data directory, it skips\u00a0over most of the container initialization scripts and will not run. A common mistake when trying different container MySQL configuration options is to rerun\u00a0`docker run`\u00a0without deleting the data directory. Since the MySQL initialization doesn't run, nothing will have changed and it won't be clear why the behavior isn't changing.\n\nTherefore, to try a different set of MySQL options, execute the following command:\n\n```", "```js\\1\n\nThis executes the MySQL CLI client inside the newly created container. There are a few commands we can run to check the status of the\u00a0`root`\u00a0and\u00a0`userauth`\u00a0user IDs. These include the following:\n\n```", "```js\\1\n\nThis says that the\u00a0`userauth`\u00a0user has full access to the\u00a0`userauth`\u00a0database. The\u00a0`root`\u00a0user, on the other hand, has full access to every database and has so many permissions that the output of that does not fit here. Fortunately, the\u00a0`root`\u00a0user is only allowed to connect from\u00a0`localhost`.\n\nTo verify this, try connecting from different locations using these commands:\n\n```", "```js\\1\n\nThis demonstrates that if the container is not attached to\u00a0`authnet`, it cannot access the MySQL server because the\u00a0`db-userauth`\u00a0hostname is not even known.\n\nWhere did the `db-userauth`\u00a0hostname\u00a0come from? We can find out by inspecting a few things:\n\n```", "```js\\1\n\nThe\u00a0`FROM`\u00a0command specifies a pre-existing image, called the base image, from which to derive a given image. Frequently, you define a Docker image by starting from an existing image. In this case, we're using the official\u00a0Node.js\u00a0Docker image ([https://hub.docker.com/_/node/](https://hub.docker.com/_/node/)), which, in turn, is\u00a0derived from\u00a0`debian`.\n\nBecause the base image,\u00a0`node`, is derived from the\u00a0`debian`\u00a0image, the commands available are what are provided on a Debian OS. Therefore, we use\u00a0`apt-get`\u00a0to install more packages.\u00a0\n\nThe\u00a0`RUN`\u00a0commands are where we run the shell commands required to build the container. The first one installs required Debian packages, such as the\u00a0`build-essential`\u00a0package, which brings in compilers required to install native-code Node.js packages.\n\nIt's recommended that you always combine\u00a0`apt-get update`,\u00a0`apt-get upgrade`, and\u00a0`apt-get install`\u00a0in the same command line like this because of the Docker build cache. Docker saves each step of the build to avoid rerunning steps unnecessarily. When rebuilding an image, Docker starts with the first changed step. Therefore, in the set of Debian packages to install changes, we want all three of those commands to run.\n\nCombining them into a single command ensures that this will occur.\u00a0For a complete discussion, refer to the documentation at\u00a0[https://docs.docker.com/develop/develop-images/dockerfile_best-practices/](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/).\n\nThe\u00a0`ENV`\u00a0commands define environment variables. In this case, we're using the same environment variables that were defined in the\u00a0`package.json`\u00a0script for launching the user authentication service.\n\nNext, we have a sequence of lines to create the\u00a0`/userauth`\u00a0directory and to populate it with the source code of the user authentication service. The first line creates the\u00a0`/userauth`\u00a0directory. The\u00a0`COPY`\u00a0command, as its name implies, copies the files for the authentication service into that directory. The\u00a0`WORKDIR`\u00a0command changes the working directory to\u00a0`/userauth`. This means that the last `RUN` command, `npm install`, is executed in\u00a0`/userauth`, and therefore, it installs the packages described in\u00a0`/userauth/package.json`\u00a0in\u00a0`/userauth/node_modules`.\n\nThere is a new\u00a0`SEQUELIZE_CONNECT`\u00a0configuration file mentioned:\u00a0`sequelize-docker-mysql.yaml`. This will describe the Sequelize configuration required to connect to the database in the\u00a0`db-userauth`\u00a0container.\n\nCreate a\u00a0new\u00a0file named\u00a0`users/sequelize-docker-mysql.yaml`\u00a0containing the following:\n\n```", "```js\\1\n\nAs has been our habit, this is an administrative task that we can record in `package.json`,\u00a0making it easier to automate this task.\n\nWe can build the authentication service as follows:\n\n```", "```js\\1\n\nThis is the set of commands that were found to be useful to manage building the images, starting the containers, and stopping the containers.\n\nLook carefully and you will see that we've added\u00a0`--detach`\u00a0to the\u00a0`docker run`\u00a0commands. So far, we've used\u00a0`docker run`\u00a0without that option, and the container remained in the foreground. While this was useful to see the logging output, it's not so useful for deployment. With the\u00a0`--detach`\u00a0option, the container becomes a background task.\n\nOn Windows, for the\u00a0-`-mount`\u00a0option, we need to change the\u00a0`src=\u00a0parameter` (as discussed earlier) to use a Windows-style hard-coded path.\u00a0That means it should read:\n\n```", "```js\\1\n\nThese commands build the pieces required for the user authentication service. As a side effect, the containers are automatically executed and will launch as background tasks.\n\nOnce it is running, you can test it using the\u00a0`cli.mjs`\u00a0script as before. You can shell into the\u00a0`svc-userauth`\u00a0container and run\u00a0`cli.mjs`\u00a0there; or, since the port is visible to the host computer, you can run it from outside the container.\n\nAfterward, we can manage the whole service as follows:\n\n```", "```js\\1\n\nThis prints out a large JSON object describing the network, along with its attached containers, which we've looked at before. If everything went well, we will see that there are now\u00a0two\u00a0containers attached to\u00a0`authnet`\u00a0where there'd\u00a0previously have just\u00a0been one.\n\nLet's go into the\u00a0`svc-userauth`\u00a0container and poke around:\n\n```", "```js\\1\n\nWe can run the\u00a0`cli.mjs`\u00a0script to test and administer the service. To get these database entries set up, use the `add` command with the appropriate options:\n\n```", "```js\\1\n\nFrom outside the containers, on the host system, we cannot ping the containers. That's because they are attached to `authnet`\u00a0and are not reachable.\n\nWe have successfully Dockerized the user authentication service in two containers\u2014`db-userauth`\u00a0and\u00a0`svc-userauth`. We've poked around the insides of a running container and found some interesting things. However, our users need the fantastic Notes application to be running, and we can't afford to rest on our laurels.\n\nSince this was our first time setting up a Docker service, we went through a lot of details. We started by launching a MySQL database container, and what is required to ensure that the data directory is persistent. We then set up a Dockerfile for the authentication service and learned how to connect containers to a common Docker network and how containers can communicate with each other over the network. We also studied the security benefits of this network infrastructure, since we can easily wall off the service and its database from intrusion.\n\nLet's now move on and Dockerize the Notes application, making sure that it is connected to the authentication server.\n\n# Creating FrontNet for the Notes application\n\nWe have the back half of our system set up in\u00a0Docker\u00a0containers, as well as the\u00a0private bridge network to connect the backend containers. It's now time to do the same for the front half of the system: the Notes application (`svc-notes`) and its associated database (`db-notes`). Fortunately, the tasks required to build FrontNet are more or less the same as what we did for AuthNet.\n\nThe first task is to set\u00a0up another private bridge network,\u00a0`frontnet`. Like\u00a0`authnet`, this will be the infrastructure for the front half of the Notes application stack.\n\nCreate a directory,\u00a0`frontnet`, and in that directory, create a\u00a0`package.json`\u00a0file that will contain the scripts to manage\u00a0`frontnet`:\u00a0\n\n```", "```js\\1\n\nWe have two virtual bridge networks. Over the next few sections, we'll set up the database and Notes application containers, connect them to `frontnet`, and then see how to manage everything.\n\n## MySQL container for the\u00a0Notes application\n\nAs with\u00a0`authnet`, the task is to construct a MySQL server container using the\u00a0`mysql/mysql-server`\u00a0image. We must configure the server to be compatible with the `SEQUELIZE_CONNECT` file that we'll use in the `svc-notes` container. For that purpose, we'll use a database named\u00a0`notes`\u00a0and a\u00a0`notes`\u00a0user ID.\n\nFor that purpose, add the following to the\u00a0`scripts`\u00a0section of the\u00a0`package.json`\u00a0file:\n\n```", "```js\\1\n\nThis database will be available in the\u00a0`db-notes`\u00a0domain name on\u00a0`frontnet`. Because it's attached to\u00a0`frontnet`, it won't be reachable by containers connected to\u00a0`authnet`. To verify this, run the following command:\n\n```", "```js\\1\n\nThis is similar to the\u00a0Dockerfile\u00a0we used for the authentication service. We're using the environment variables from\u00a0`notes/package.json`, plus a new one:\u00a0`NOTES_SESSION_DIR`.\n\nThe most obvious change is the number of\u00a0`COPY`\u00a0commands. The Notes application is a lot more involved, given the number of sub-directories full of files that must be installed. We start by creating the top-level directories of the Notes application deployment tree. Then, one by one, we copy each sub-directory into its corresponding sub-directory in the container filesystem.\n\nIn a\u00a0`COPY`\u00a0command, the trailing slash on the destination directory is important. Why? Because the Docker documentation says that the trailing slash is important, that's why.\n\nThe big question is *why use multiple\u00a0*`COPY`*\u00a0commands like this*? This would have been incredibly simple:\n\n```", "```js\\1\n\nThis will access a database server on the\u00a0`db-notes`\u00a0domain name using the named database, username, and password.\u00a0\n\nNotice that the\u00a0`USER_SERVICE_URL`\u00a0variable no longer accesses the authentication service at\u00a0`localhost`, but at\u00a0`svc-userauth`. The\u00a0`svc-userauth`\u00a0domain name is currently\u00a0only advertised by the DNS server on AuthNet, but the Notes service is on FrontNet. Therefore, this will cause a failure for us when we get to running the Notes application, and we'll have to make some connections so that the\u00a0`svc-userauth`\u00a0container can be accessed from\u00a0`svc-notes`.\n\nIn\u00a0[Chapter 8](1ef2de06-5b7d-44c8-a132-55f822d113cf.xhtml)*,*\u00a0*Authenticating Users with a Microservice*,\u00a0we discussed the need to protect the API keys supplied by Twitter. We could copy the\u00a0`.env`\u00a0file\u00a0to the\u00a0Dockerfile, but this may not be the best choice, and so we've left it out of the Dockerfile.\n\nUnfortunately, this does not protect the Twitter credentials to the level required. The\u00a0`.env`\u00a0file is available as plaintext inside the container. Docker has a feature, Docker Secrets, that can be used to securely store data of this sort. Unfortunately, it is only available when using Swarm mode, which we are not doing at this time; but we will use this feature in [Chapter 12](8551a26c-6834-4df6-b392-60a15c20f6ff.xhtml),\u00a0*Deploying a Docker Swarm to AWS EC2 Using Terraform*.\n\nThe value of\u00a0`TWITTER_CALLBACK_HOST`\u00a0needs to reflect where Notes is deployed. Right now, it is still on your laptop, but if it is deployed to a server, this variable will require the IP address or domain name of the server.\n\nIn\u00a0`notes/package.json`,\u00a0add the following\u00a0`scripts` entry:\n\n```", "```js\\1\n\nNow, we can build the container image:\n\n```", "```js\\1\n\nWe can reach\u00a0`db-notes`\u00a0from\u00a0`svc-notes`\u00a0but not\u00a0`svc-userauth`. This is as expected since we have attached these containers to different networks.\n\nIf you inspect FrontNet and AuthNet, you'll see that the containers attached to each do not overlap:\n\n```", "```js\\1\n\nWith no other change, the Notes application will now allow you to log in and start adding and editing notes. Furthermore, start a shell in\u00a0`svc-notes`\u00a0and you'll be able to ping both\u00a0`svc-userauth`\u00a0and\u00a0`db-userauth`.\n\nThere is a glaring architecture question staring at us. Do we connect the\u00a0`svc-userauth`\u00a0service to\u00a0`frontnet`, or do we connect the\u00a0`svc-notes`\u00a0service to\u00a0`authnet`? We just connected `svc-notes` to `authnet`, but maybe that's not the best choice.\u00a0To verify which network setup solves the problem, run the following commands:\n\n```", "```js\\1\n\nThis sequence reconnects\u00a0`svc-notes`\u00a0to\u00a0`authnet`\u00a0and demonstrates the ability to access both the\u00a0`svc-userauth`\u00a0and\u00a0`db-userauth`\u00a0containers. Therefore, a successful invader could access the\u00a0`db-userauth`\u00a0database, a result we wanted to prevent. Our diagram in\u00a0[Chapter 10](176ce11c-dd6f-4ebf-ba14-529be6db28da.xhtml),\u00a0*Deploying Node.js Applications to Linux Servers,*\u00a0showed no such connection between\u00a0`svc-notes`\u00a0and\u00a0`db-userauth`.\n\nGiven that our goal for using Docker was to limit the attack vectors, we have a clear distinction between the two container/network connection setups. Attaching\u00a0`svc-userauth`\u00a0to\u00a0`frontnet`\u00a0limits the number of containers that can access\u00a0`db-userauth`. For an intruder to access the user information database, they must first break into\u00a0`svc-notes`, and then break into\u00a0`svc-userauth`; unless, that is, our amateur attempt at a security audit is flawed.\n\nFor this and a number of other reasons, we arrive at this final set of\u00a0scripts\u00a0for\u00a0`frontnet/package.json`:\n\n```", "```js\\1\n\nBecause we've automated many things, it is this simple to administer the system. However, it is not as automated as we want it to be. To address that, let's\u00a0learn how to make the Notes stack more easily deployable by using Docker Compose to describe the infrastructure.\n\n# Managing multiple containers with Docker Compose\n\nIt is cool that we can create encapsulated\u00a0instantiations\u00a0of the software services that we've created. In theory, we can publish these images to Docker repositories, and then launch the containers on any server we want. For example, our task in [Chapter 10](176ce11c-dd6f-4ebf-ba14-529be6db28da.xhtml),\u00a0*Deploying Node.js Applications to Linux Servers*, would be greatly simplified with Docker. We could simply install Docker Engine on the Linux host and then deploy our containers on that server, and not have to deal with all those scripts and the PM2 application.\n\nBut we haven't properly automated the process. The promise was to use the Dockerized application for deployment on cloud services. In other words, we need to take all this learning and apply it to the task of simplifying deployment.\n\nWe've demonstrated that, with Docker, Notes can be built using four containers that have a high degree of isolation from each other and from the outside world.\u00a0\n\nThere is a glaring problem: our process in the previous section was partly manual, partly automated. We created scripts to launch each portion of the system, which is good practice. However, we did not automate the entire process to bring up Notes and the authentication services, nor is this solution scalable beyond one machine.\n\nLet's start with the last issue first\u2014scalability. Within the Docker ecosystem, several\u00a0**Docker orchestrator**\u00a0services are available. An orchestrator automatically deploys and manages\u00a0Docker\u00a0containers over a group of machines. Some examples of Docker orchestrators are Docker Swarm, Kubernetes, CoreOS Fleet, and Apache Mesos. These are powerful systems that can automatically increase/decrease resources as needed to move containers from one host to another, and more. We mention these systems for you to further study as your needs grow. In [Chapter 12](8551a26c-6834-4df6-b392-60a15c20f6ff.xhtml),\u00a0*Deploying a Docker Swarm to AWS EC2 with Terraform*, we will build on the work we're about to do in order to deploy Notes in a Docker Swarm cluster that we'll build on AWS EC2 infrastructure.\n\nDocker\u00a0Compose ([https://docs.docker.com/compose/overview/](https://docs.docker.com/compose/overview/)) will solve the other problems we've identified. It lets us easily define and run several Docker containers together as a complete application. It uses a YAML file,\u00a0`docker-compose.yml`, to describe the containers, their dependencies, the virtual networks, and the volumes. While we'll be using it to describe deployment on a single host machine, Docker Compose can be used for multi-machine deployments. Namely, Docker Swarm directly uses compose files to describe the services you launch in a swarm. In any case, learning about Docker Compose will give you a headstart on understanding the other systems.\n\nBefore proceeding, ensure that Docker Compose is installed. If you've installed Docker for Windows or Docker for Mac, everything that is required is installed. On Linux, you must install it separately by following the instructions in the links provided earlier.\n\n## Docker Compose file for the Notes stack\n\nWe just talked about Docker orchestration services, but Docker Compose is not itself such a service. Instead, Docker Compose uses a specific YAML file structure to describe how to deploy Docker containers. With a Docker Compose file, we can describe one or more containers, networks, and volumes involved in launching a Docker-based service.\n\nLet's start by\u00a0creating\u00a0a directory,\u00a0`compose-local`, as a sibling to the\u00a0`users`\u00a0and\u00a0`notes`\u00a0directories. In that directory, create a file named\u00a0`docker-compose.yml`:\n\n```", "```js\\1\n\nWe first needed to stop and delete any existing containers left over from our previous work. We can also use the scripts in the `frontnet` and `authnet` directories to do this. `docker-compose.yml`\u00a0used the same container names, so we need the ability to launch new containers with those names.\n\nTo get started, use this command:\n\n```", "```js\\1\n\nWe can use `docker-compose stop`\u00a0to shut down the containers. With `docker-compose start`, the containers run in the background.\n\nWe can also run\u00a0`docker-compose up`\u00a0to get a different experience:\n\n```", "```js\\1\n\nThis is related to running\u00a0`docker ps`, but the presentation is a little different and more compact.\n\nIn\u00a0`docker-compose.yml`,\u00a0we insert the following declaration for\u00a0`svc-userauth`:\n\n```", "```js\\1\n\nWe started the Docker containers using\u00a0`docker-compose`, and we can use the\u00a0`docker-compose`\u00a0command to interact with the containers. In this case, we demonstrated using both the\u00a0`docker-compose`\u00a0and `docker`\u00a0commands to execute a command inside one of the containers. While there are slight differences in the command syntax, it's the same interaction with the same results.\n\nAnother test is to go into the containers and explore:\n\n```", "```js\\1\n\n`svc-userauth`\u00a0is no longer\u00a0connected to\u00a0`frontnet`, which is how we could ping\u00a0`db-notes`\u00a0from\u00a0`svc-userauth`. Instead,\u00a0`svc-userauth`\u00a0and\u00a0`svc-notes`\u00a0are both connected to a new network,\u00a0`svcnet`, which is meant to connect the service containers. Therefore, both service containers have exactly the required access to match the goals outlined at the beginning.\n\nThat's an advantage of Docker Compose. We can quickly reconfigure the system without rewriting anything other than the\u00a0`docker-compose.yml`\u00a0configuration file. Furthermore, the new configuration is instantly reflected in a file that can be committed to our source repository.\n\nWhen you're done testing the system, simply type\u00a0*CTRL*\u00a0+*\u00a0C*\u00a0in the terminal:\n\n```", "```js\\1\n\nThe\u00a0`docker-compose`\u00a0commands\u2014`start`,\u00a0`stop`*,*\u00a0and\u00a0`restart`\u2014all serve as ways to manage the containers as background tasks. The default mode for the\u00a0`docker-compose up`\u00a0command is, as we've seen, to start the containers in the foreground. However, we can also run\u00a0`docker-compose up`\u00a0with the\u00a0`-d`\u00a0option, which says to detach the containers from the terminal to run in the background.\n\nWe're getting closer to our end goal. In this section, we learned how to take the Docker containers we've designed and create a system that can be easily brought up and down as a unit by running the\u00a0`docker-compose`\u00a0command.\n\nWhile preparing to deploy this to Docker Swarm on AWS EC2, a horizontal scaling issue was found, which we can fix on our laptop. It is fairly easy with Docker Compose files to test multiple\u00a0`svc-notes`\u00a0instances to see whether we can scale Notes for higher traffic loads. Let's take a look at that before deploying to the swarm.\n\n# Using Redis for scaling the Notes application stack\n\nIn the previous section, we learned how to use Docker Compose to manage\u00a0the Notes application stack. Looking ahead, we can see the potential need to use multiple instances of the Notes container when we deploy to Docker Swarm on AWS EC2\\. In this section, we will make a small modification to the Docker Compose file for an ad hoc test with multiple Notes containers. This test will show us a couple of problems. Among the available solutions are two packages that fix both problems by installing a Redis instance.\n\nA common tactic for handling high traffic loads is to deploy multiple service instances as needed. This is called horizontal scaling, where we deploy multiple instances of a service to multiple servers. What we'll do in this section is learn a little about horizontal scaling in Docker by starting two Notes instances to see how it behaves.\n\nAs it currently exists, Notes stores some data\u2014the session data\u2014on the local disk space. As orchestrators such as Docker Swarm, ECS, and Kubernetes scale containers up and down, containers are constantly created and destroyed or moved from one host to another. This is done in the name of handling the traffic while optimizing the load on the available servers. In this case, whatever active data we're storing on a local disk will be lost. Losing the session data means users will be randomly logged out. The users will be rightfully upset\u00a0and will then send us support requests asking what's wrong and whether\u00a0we have even tested this thing!\n\nIn this section, we will learn that Notes does not behave well when we have multiple instances of `svc-notes`. To address this problem, we will add a Redis container to the Docker Compose setup and configure Notes to use Redis to solve the two problems that we have discovered. This will\u00a0ensure that the session data is shared between multiple Notes instances via a Redis server.\n\nLet's get started by performing a little ad hoc testing to better understand the problem.\n\n## Testing session management with multiple Notes service instances\n\nWe can easily verify whether Notes properly handles session data if there are multiple\u00a0`svc-notes`\u00a0instances. With a small modification to\u00a0`compose-local/docker-compose.yml`,\u00a0we can start two\u00a0`svc-notes`\u00a0instances, or more. They'll be on separate TCP ports, but it will let us see how Notes behaves with multiple instances of the Notes service.\n\nCreate a new service,\u00a0`svc-notes-2`, by duplicating the\u00a0`svc-notes`\u00a0declaration. The only thing to change is the container name, which should be\u00a0`svc-notes-2`, and the published port, which should be port\u00a0`3020`.\n\nFor example, add the following to\u00a0`compose-local/docker-compose.yml`:\n\n```", "```js\\1\n\nIn this case, there was no source code change, only a configuration change. Therefore, the containers do not need to be rebuilt, and we can simply relaunch with the new configuration.\n\nThat will give us two Notes containers on different ports. Each is configured as normal; for example, they connect to the same user authentication service. Using two browser windows, visit both at their respective port numbers. You'll be able to log in with one browser window, but you'll encounter the following situation:\n\n![](img/642669ba-4d37-4eda-b870-a164aa968cc7.png)\n\nThe browser window on port\u00a0`3020`\u00a0is logged out, while the window open to port `3000` is logged in. Remember that port\u00a0`3020`\u00a0is\u00a0`svc-notes-2`, while port `3000` is\u00a0`svc-notes`. However, as you use the two windows, you'll observe some flaky behavior with regard to staying logged in.\u00a0\n\nThe issue is that the session data is not shared between\u00a0`svc-notes`\u00a0and\u00a0`svc-notes-2`. Instead, the session data is in files stored within each container.\n\nWe've identified a problem whereby keeping the session data inside the container makes it impossible to share session data across all instances of the Notes service. To fix this, we need a session store that shares the session data across processes.\n\n## Storing Express/Passport\u00a0session data in a Redis server\n\nLooking back, we saw that we might have multiple instances of\u00a0`svc-notes`\u00a0deployed on Docker Swarm. To test this, we created a second instance,\u00a0`svc-notes-2`, and found that user sessions were not maintained between the two Notes instances. This told us that we must store session data in a shared data storage system.\n\nThere are several choices when it comes to storing sessions. While it is tempting to use the\u00a0`express-session-sequelize`\u00a0package, because we're already using Sequelize to manage a database, we have another issue to solve that requires the use of Redis. We'll discuss this other issue later.\n\nFor a list of Express session stores, go to\u00a0[http://expressjs.com/en/resources/middleware/session.html#compatible-session-stores](http://expressjs.com/en/resources/middleware/session.html#compatible-session-stores).\n\nRedis is a widely used key-value data store that is known for being very fast. It is also very easy to install and use. We won't have to learn anything about Redis, either.\n\nSeveral steps are required in order to set up Redis:\n\n1.  In\u00a0`compose-local/docker-compose.yml`, add the following definition to the\u00a0`services`\u00a0section:\n\n```", "```js\\1\n\nAdd this to both the\u00a0`svc-notes`\u00a0and\u00a0`svc-notes-2`\u00a0service declarations. This passes the Redis hostname to the Notes service.\n\n3.  Next, install the package:\n\n```", "```js\\1\n\nThis brings in the Redis-based session store provided by\u00a0`connect-redis`.\n\nThe configuration for these packages is taken directly from the relevant documentation.\n\nFor\u00a0`connect-redis`, refer to\u00a0[https://www.npmjs.com/package/connect-redis](https://www.npmjs.com/package/connect-redis). [](https://www.npmjs.com/package/connect-redis) For\u00a0`redis`, refer to [https://github.com/NodeRedis/node-redis](https://github.com/NodeRedis/node-redis).\n\nThis imports the two packages and then configures the\u00a0`connect-redis`\u00a0package to use the\u00a0`redis`\u00a0package. We consulted the\u00a0`REDIS_ENDPOINT`\u00a0environment variable to configure the `redis` client object. The result landed in the same\u00a0`sessionStore`\u00a0variable we used previously. Therefore, no other change is required in\u00a0`app.mjs`.\n\nIf no Redis endpoint is specified, we instead revert to the file-based session store. We might not always deploy Notes in a context where we can run Redis; for example, while developing on our laptop. Therefore, we require the option of not using Redis, and, at the moment, the choice looks to be between using Redis or the filesystem to store session data.\n\nWith these changes, we can relaunch the Notes application stack. It might help to relaunch the stack using the following command:\n\n```", "```js\\1\n\nThis installs the\u00a0`socket.io-redis`\u00a0package.\n\nThen, we configure it in\u00a0`app.mjs`,\u00a0as follows:\n\n```"]